{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1276ed9-da18-41b4-af09-c4907de76c46",
   "metadata": {},
   "source": [
    "## Restaurant NER (AE - Aspect Extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec683a-f5bc-4e3f-8c6d-b685bf043a50",
   "metadata": {},
   "source": [
    "### Preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bfe08b4-aa9c-426d-b68b-db02399e312d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1850"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "rest_ner_training_file = '/Users/sean.huang/Documents/my/books and courses/ztgg/Shared Code/BERT-for-ABSA/ae/rest/train.json'\n",
    "rest_ner_training_samples = json.load(open(rest_ner_training_file))\n",
    "len(rest_ner_training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2073e5c4-4b22-4b49-9d6f-4d9f1e7e97d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': ['O', 'O', 'O', 'B'], 'sentence': ['I', 'LOVE', 'their', 'Thai']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_ner_training_samples['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bda66adf-796f-4ef1-94d7-eee6362023fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[I, LOVE, their, Thai]</td>\n",
       "      <td>[O, O, O, B]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[The, service, was, attentive, .]</td>\n",
       "      <td>[O, B, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[I, go, twice, a, month, !]</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[The, food, was, average, to, above, -, averag...</td>\n",
       "      <td>[O, B, O, O, O, O, O, O, O, O, B, I, I, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Ask, for, Usha, ,, the, nicest, bartender, in...</td>\n",
       "      <td>[O, O, B, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845</th>\n",
       "      <td>1845</td>\n",
       "      <td>[You, can, get, a, table, without, a, reservat...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>1846</td>\n",
       "      <td>[I, have, eaten, at, some, of, the, 'best, ', ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>1847</td>\n",
       "      <td>[Worth, visiting, the, 1st, Ave, spot, because...</td>\n",
       "      <td>[O, O, O, B, I, I, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>1848</td>\n",
       "      <td>[Its, located, in, greenewich, village, .]</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>1849</td>\n",
       "      <td>[Overall, the, food, quality, was, pretty, goo...</td>\n",
       "      <td>[O, O, B, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1850 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_id                                           sentence  \\\n",
       "0              0                             [I, LOVE, their, Thai]   \n",
       "1              1                  [The, service, was, attentive, .]   \n",
       "2              2                        [I, go, twice, a, month, !]   \n",
       "3              3  [The, food, was, average, to, above, -, averag...   \n",
       "4              4  [Ask, for, Usha, ,, the, nicest, bartender, in...   \n",
       "...          ...                                                ...   \n",
       "1845        1845  [You, can, get, a, table, without, a, reservat...   \n",
       "1846        1846  [I, have, eaten, at, some, of, the, 'best, ', ...   \n",
       "1847        1847  [Worth, visiting, the, 1st, Ave, spot, because...   \n",
       "1848        1848         [Its, located, in, greenewich, village, .]   \n",
       "1849        1849  [Overall, the, food, quality, was, pretty, goo...   \n",
       "\n",
       "                                                  label  \n",
       "0                                          [O, O, O, B]  \n",
       "1                                       [O, B, O, O, O]  \n",
       "2                                    [O, O, O, O, O, O]  \n",
       "3     [O, B, O, O, O, O, O, O, O, O, B, I, I, O, O, ...  \n",
       "4                        [O, O, B, O, O, O, O, O, O, O]  \n",
       "...                                                 ...  \n",
       "1845  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1846  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1847            [O, O, O, B, I, I, O, O, O, O, O, O, O]  \n",
       "1848                                 [O, O, O, O, O, O]  \n",
       "1849  [O, O, B, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "\n",
       "[1850 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_rest_ner_training_samples = {'sentence_id':[], 'sentence':[], 'label':[]}\n",
    "\n",
    "for key in rest_ner_training_samples.keys():\n",
    "    dict_rest_ner_training_samples['sentence_id'].append(key)\n",
    "    dict_rest_ner_training_samples['sentence'].append(rest_ner_training_samples[key]['sentence'])\n",
    "    dict_rest_ner_training_samples['label'].append(rest_ner_training_samples[key]['label'])\n",
    "    \n",
    "dict_rest_ner_training_samples\n",
    "\n",
    "df_rest_ner_training_samples = pd.DataFrame.from_dict(dict_rest_ner_training_samples)\n",
    "df_rest_ner_training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42733d52-10b5-4cbc-b14a-e87be666f8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>LOVE</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>their</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thai</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>1849</td>\n",
       "      <td>front</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>1849</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>1849</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>1849</td>\n",
       "      <td>guest</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>1849</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26957 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_id sentence label\n",
       "0              0        I     O\n",
       "0              0     LOVE     O\n",
       "0              0    their     O\n",
       "0              0     Thai     B\n",
       "1              1      The     O\n",
       "...          ...      ...   ...\n",
       "1849        1849    front     O\n",
       "1849        1849       of     O\n",
       "1849        1849      the     O\n",
       "1849        1849    guest     O\n",
       "1849        1849        .     O\n",
       "\n",
       "[26957 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rest_ner_training_samples_exploded = df_rest_ner_training_samples.explode(['sentence', 'label'])\n",
    "df_rest_ner_training_samples_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6fb3a6a-5f5c-47ec-b66b-1462e191a0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Dumbfoundingly', 0), ('as', 1), ('fix', 2), ('Yamato', 3), ('extra', 4)] [(0, 'Dumbfoundingly'), (1, 'as'), (2, 'fix'), (3, 'Yamato'), (4, 'extra')]\n",
      "{'I': 0, 'O': 1, 'B': 2} {0: 'I', 1: 'O', 2: 'B'}\n"
     ]
    }
   ],
   "source": [
    "def get_mapping(data, column):\n",
    "    t2id = {}\n",
    "    id2t = {}\n",
    " \n",
    "    vocab = list(set(data[column].to_list()))\n",
    "    \n",
    "    id2t = {i:t for i, t in enumerate(vocab)}\n",
    "    t2id = {t:i for  i, t in enumerate(vocab)}\n",
    "    return t2id, id2t\n",
    "\n",
    "\n",
    "token2id, id2token = get_mapping(df_rest_ner_training_samples_exploded, 'sentence')\n",
    "tag2id, id2tag = get_mapping(df_rest_ner_training_samples_exploded, 'label')\n",
    "\n",
    "print(list(token2id.items())[0:5], list(id2token.items())[0:5])\n",
    "print(tag2id, id2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30140871-d211-4a4d-bc46-176a01dada7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>word_id</th>\n",
       "      <th>tag_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>O</td>\n",
       "      <td>496</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>LOVE</td>\n",
       "      <td>O</td>\n",
       "      <td>2564</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>their</td>\n",
       "      <td>O</td>\n",
       "      <td>1248</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thai</td>\n",
       "      <td>B</td>\n",
       "      <td>296</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "      <td>1194</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>service</td>\n",
       "      <td>B</td>\n",
       "      <td>3339</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>was</td>\n",
       "      <td>O</td>\n",
       "      <td>2107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>attentive</td>\n",
       "      <td>O</td>\n",
       "      <td>1797</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>2193</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I</td>\n",
       "      <td>O</td>\n",
       "      <td>496</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>go</td>\n",
       "      <td>O</td>\n",
       "      <td>3334</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>twice</td>\n",
       "      <td>O</td>\n",
       "      <td>2652</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "      <td>1176</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>month</td>\n",
       "      <td>O</td>\n",
       "      <td>3425</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>!</td>\n",
       "      <td>O</td>\n",
       "      <td>1455</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "      <td>1194</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>food</td>\n",
       "      <td>B</td>\n",
       "      <td>231</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>was</td>\n",
       "      <td>O</td>\n",
       "      <td>2107</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>average</td>\n",
       "      <td>O</td>\n",
       "      <td>844</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "      <td>930</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_id   sentence label  word_id  tag_id\n",
       "0           0          I     O      496       1\n",
       "0           0       LOVE     O     2564       1\n",
       "0           0      their     O     1248       1\n",
       "0           0       Thai     B      296       2\n",
       "1           1        The     O     1194       1\n",
       "1           1    service     B     3339       2\n",
       "1           1        was     O     2107       1\n",
       "1           1  attentive     O     1797       1\n",
       "1           1          .     O     2193       1\n",
       "2           2          I     O      496       1\n",
       "2           2         go     O     3334       1\n",
       "2           2      twice     O     2652       1\n",
       "2           2          a     O     1176       1\n",
       "2           2      month     O     3425       1\n",
       "2           2          !     O     1455       1\n",
       "3           3        The     O     1194       1\n",
       "3           3       food     B      231       2\n",
       "3           3        was     O     2107       1\n",
       "3           3    average     O      844       1\n",
       "3           3         to     O      930       1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rest_ner_training_samples_exploded['word_id'] = df_rest_ner_training_samples_exploded['sentence'].map(token2id)\n",
    "df_rest_ner_training_samples_exploded['tag_id'] = df_rest_ner_training_samples_exploded['label'].map(tag2id)\n",
    "df_rest_ner_training_samples_exploded.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ba851be-18b6-483d-bd1d-b9bad715c101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    0\n",
       "label       0\n",
       "word_id     0\n",
       "tag_id      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rest_ner_training_samples_exploded.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "63831610-a097-44a3-a406-d3d2bd9487a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/57/qcnnk6sn1vvbtwg1g1xc6kmw0000gp/T/ipykernel_47302/3037838837.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  df_rest_ner_training_samples = df_rest_ner_training_samples_exploded.groupby(['sentence_id'])['sentence', 'label', 'word_id', 'tag_id'].agg(lambda x: list(x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>word_id</th>\n",
       "      <th>tag_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[I, LOVE, their, Thai]</td>\n",
       "      <td>[O, O, O, B]</td>\n",
       "      <td>[529, 3167, 1628, 2790]</td>\n",
       "      <td>[2, 2, 2, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, service, was, attentive, .]</td>\n",
       "      <td>[O, B, O, O, O]</td>\n",
       "      <td>[2898, 115, 2073, 554, 2835]</td>\n",
       "      <td>[2, 1, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[Everything, is, always, cooked, to, perfectio...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B, O, O, O, O, B, O, ...</td>\n",
       "      <td>[313, 109, 2381, 2296, 3308, 2946, 3509, 1261,...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>[good, music, ,, great, food, ,, speedy, servi...</td>\n",
       "      <td>[O, B, O, O, B, O, O, B, O, O, O]</td>\n",
       "      <td>[687, 1927, 3509, 3143, 725, 3509, 903, 115, 7...</td>\n",
       "      <td>[2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>[But, the, pizza, is, way, to, expensive, .]</td>\n",
       "      <td>[O, O, B, O, O, O, O, O]</td>\n",
       "      <td>[891, 1261, 1812, 109, 1749, 3308, 32, 2835]</td>\n",
       "      <td>[2, 2, 1, 2, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>[The, only, beverage, we, did, receive, was, w...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[2898, 899, 479, 3270, 3263, 2692, 2073, 466, ...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>[A, friend, from, Seattle, and, I, went, on, a...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[3077, 1371, 2882, 739, 3425, 529, 1245, 2952,...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>[Went, on, a, 3, day, oyster, binge, ,, with, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B, O, O, O, O, O, ...</td>\n",
       "      <td>[2427, 2952, 2363, 3047, 3312, 401, 1111, 3509...</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>[Service, is, fast, and, friendly, .]</td>\n",
       "      <td>[B, O, O, O, O, O]</td>\n",
       "      <td>[3687, 109, 393, 3425, 452, 2835]</td>\n",
       "      <td>[1, 2, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>[I, ordered, the, vitello, alla, marsala, and,...</td>\n",
       "      <td>[O, O, O, B, I, I, O, O, O, O, O, O]</td>\n",
       "      <td>[529, 2478, 1261, 3151, 2614, 152, 3425, 529, ...</td>\n",
       "      <td>[2, 2, 2, 1, 0, 0, 2, 2, 2, 2, 2, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1850 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      sentence  \\\n",
       "sentence_id                                                      \n",
       "0                                       [I, LOVE, their, Thai]   \n",
       "1                            [The, service, was, attentive, .]   \n",
       "10           [Everything, is, always, cooked, to, perfectio...   \n",
       "100          [good, music, ,, great, food, ,, speedy, servi...   \n",
       "1000              [But, the, pizza, is, way, to, expensive, .]   \n",
       "...                                                        ...   \n",
       "995          [The, only, beverage, we, did, receive, was, w...   \n",
       "996          [A, friend, from, Seattle, and, I, went, on, a...   \n",
       "997          [Went, on, a, 3, day, oyster, binge, ,, with, ...   \n",
       "998                      [Service, is, fast, and, friendly, .]   \n",
       "999          [I, ordered, the, vitello, alla, marsala, and,...   \n",
       "\n",
       "                                                         label  \\\n",
       "sentence_id                                                      \n",
       "0                                                 [O, O, O, B]   \n",
       "1                                              [O, B, O, O, O]   \n",
       "10           [O, O, O, O, O, O, O, O, B, O, O, O, O, B, O, ...   \n",
       "100                          [O, B, O, O, B, O, O, B, O, O, O]   \n",
       "1000                                  [O, O, B, O, O, O, O, O]   \n",
       "...                                                        ...   \n",
       "995                       [O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "996                 [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "997          [O, O, O, O, O, O, O, O, O, B, O, O, O, O, O, ...   \n",
       "998                                         [B, O, O, O, O, O]   \n",
       "999                       [O, O, O, B, I, I, O, O, O, O, O, O]   \n",
       "\n",
       "                                                       word_id  \\\n",
       "sentence_id                                                      \n",
       "0                                      [529, 3167, 1628, 2790]   \n",
       "1                                 [2898, 115, 2073, 554, 2835]   \n",
       "10           [313, 109, 2381, 2296, 3308, 2946, 3509, 1261,...   \n",
       "100          [687, 1927, 3509, 3143, 725, 3509, 903, 115, 7...   \n",
       "1000              [891, 1261, 1812, 109, 1749, 3308, 32, 2835]   \n",
       "...                                                        ...   \n",
       "995          [2898, 899, 479, 3270, 3263, 2692, 2073, 466, ...   \n",
       "996          [3077, 1371, 2882, 739, 3425, 529, 1245, 2952,...   \n",
       "997          [2427, 2952, 2363, 3047, 3312, 401, 1111, 3509...   \n",
       "998                          [3687, 109, 393, 3425, 452, 2835]   \n",
       "999          [529, 2478, 1261, 3151, 2614, 152, 3425, 529, ...   \n",
       "\n",
       "                                                        tag_id  \n",
       "sentence_id                                                     \n",
       "0                                                 [2, 2, 2, 1]  \n",
       "1                                              [2, 1, 2, 2, 2]  \n",
       "10           [2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, ...  \n",
       "100                          [2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 2]  \n",
       "1000                                  [2, 2, 1, 2, 2, 2, 2, 2]  \n",
       "...                                                        ...  \n",
       "995                       [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]  \n",
       "996                 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]  \n",
       "997          [2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, ...  \n",
       "998                                         [1, 2, 2, 2, 2, 2]  \n",
       "999                       [2, 2, 2, 1, 0, 0, 2, 2, 2, 2, 2, 2]  \n",
       "\n",
       "[1850 rows x 4 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rest_ner_training_samples = df_rest_ner_training_samples_exploded.groupby(['sentence_id'])['sentence', 'label', 'word_id', 'tag_id'].agg(lambda x: list(x))\n",
    "df_rest_ner_training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "655e98ff-c8be-4416-a938-35364af27bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sent_length = max([len(each) for each in list(df_rest_ner_training_samples['sentence'])])\n",
    "max_sent_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e27014-3682-440e-bf76-9e896a1f84ff",
   "metadata": {},
   "source": [
    "### Load and finetune pretrained Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6fc29cb3-82c3-4696-ac4d-f52ef0e62954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /Users/sean.huang/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /Users/sean.huang/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /Users/sean.huang/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /Users/sean.huang/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /Users/sean.huang/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "pretrained_bert_model_name = 'bert-base-uncased'\n",
    "pretrained_bert_tokenizer = AutoTokenizer.from_pretrained(pretrained_bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0110e2fa-2201-45d7-ad8b-284ff3e802fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2293, 2037, 7273, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pretrained_bert_tokenizer(list(df_rest_ner_training_samples['sentence'])[0], is_split_into_words=True)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d9ac7c1f-b963-49e2-a47b-09405f4bfc22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d4fd14cf97467fbb30ff6666d8d4a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['O', 'O', 'O', 'B']\n",
      "[None, 0, 1, 2, 3, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O']\n",
      "[None, 0, 1, 2, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'B', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 1, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 16, 16, 17, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 2, 2, 3, 4, 5, 6, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'B', 'O', 'O', 'O']\n",
      "[None, 0, 1, 1, 2, 3, 4, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 1, 1, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 12, 13, 14, 15, 16, 17, 18, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 1, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 4, 4, 5, 6, 7, 8, 9, 10, 11, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c6118f7dc74e9caa6134d913542263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 0, 1, 2, 3, 4, 4, 5, 6, 7, 8, 9, 9, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'B', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 1, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['B', 'O', 'O', 'O', 'B', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 2, 2, 2, 1, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 19, 19, 20, 21, 22, 23, 24, 25, 26, 26, 27, 28, 29, 30, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, None]\n",
      "[-100, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 6, 7, 8, 9, 10, 11, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 1, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 19, 19, 20, 21, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 0, 0, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['B', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 4, 4, 5, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 1, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 9, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, None, None, None]\n",
      "[-100, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 0, 2, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, -100]\n",
      "0\n",
      "['B', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 36, 36, 37, 37, 37, None]\n",
      "[-100, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, 13, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O']\n",
      "[None, 0, 1, 2, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 7, 8, 8, 8, 9, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def align_ner_labels(samples):\n",
    "    inputs = pretrained_bert_tokenizer(samples['sentence'], truncation=True, padding='max_length', max_length=128, is_split_into_words=True)\n",
    "\n",
    "    label_input = []\n",
    "    for i, label in enumerate(samples['label']):\n",
    "        word_ids = inputs.word_ids(batch_index=i)\n",
    "        \n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(tag2id[label[word_idx]])\n",
    "        \n",
    "        if i == 0:\n",
    "            print(i)\n",
    "            print(label)\n",
    "            print(word_ids)\n",
    "            print(label_ids)\n",
    "            \n",
    "        label_input.append(label_ids)\n",
    "        \n",
    "    inputs['labels'] = label_input\n",
    "    \n",
    "    # print('inputs', inputs.keys())\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "train_datasets = Dataset.from_pandas(df_rest_ner_training_samples[:1600]).map(align_ner_labels, batched=True, batch_size=100).remove_columns(['sentence', 'label', 'word_id', 'tag_id', 'sentence_id'])\n",
    "validation_datasets = Dataset.from_pandas(df_rest_ner_training_samples[1600:]).map(align_ner_labels, batched=True, batch_size=10).remove_columns(['sentence', 'label', 'word_id', 'tag_id', 'sentence_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "16dc08e7-ae33-420c-afbe-4a961de008bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1600\n",
       "})"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "628c7f18-5372-417c-a738-44274af40711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "03af1e33-66ea-4513-b255-09bcb6724652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')   # mps is for Apple M1 GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "480076ef-b772-4d1b-8bfa-61d2a108ad7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /Users/sean.huang/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"I\",\n",
      "    \"1\": \"B\",\n",
      "    \"2\": \"O\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"B\": 1,\n",
      "    \"I\": 0,\n",
      "    \"O\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /Users/sean.huang/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import torch\n",
    "\n",
    "pretrained_bert_model = AutoModelForTokenClassification.from_pretrained(pretrained_bert_model_name, num_labels=len(tag2id.keys()), id2label=id2tag, label2id=tag2id)\n",
    "\n",
    "pretrained_bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d27c9321-93b2-44c6-8fc8-5612dad8e15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "torch.distributed process group is initialized, but local_rank == -1. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "/Users/sean.huang/miniconda3/envs/m1_new/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1600\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1600/1600 14:21, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.218634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.095442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.083480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.003500</td>\n",
       "      <td>0.103103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.006900</td>\n",
       "      <td>0.100134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.111565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.112978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.118741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.128856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.127052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./rest_ner_model/checkpoint-500\n",
      "Configuration saved in ./rest_ner_model/checkpoint-500/config.json\n",
      "Model weights saved in ./rest_ner_model/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./rest_ner_model/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./rest_ner_model/checkpoint-500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./rest_ner_model/checkpoint-1000\n",
      "Configuration saved in ./rest_ner_model/checkpoint-1000/config.json\n",
      "Model weights saved in ./rest_ner_model/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./rest_ner_model/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./rest_ner_model/checkpoint-1000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./rest_ner_model/checkpoint-1500\n",
      "Configuration saved in ./rest_ner_model/checkpoint-1500/config.json\n",
      "Model weights saved in ./rest_ner_model/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./rest_ner_model/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./rest_ner_model/checkpoint-1500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./rest_ner_model\n",
      "Configuration saved in ./rest_ner_model/config.json\n",
      "Model weights saved in ./rest_ner_model/pytorch_model.bin\n",
      "tokenizer config file saved in ./rest_ner_model/tokenizer_config.json\n",
      "Special tokens file saved in ./rest_ner_model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='./rest_ner_model',\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=10,\n",
    "    logging_strategy='epoch',\n",
    "    evaluation_strategy='epoch',\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(pretrained_bert_tokenizer)\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model=pretrained_bert_model,\n",
    "    args=args,\n",
    "    train_dataset=train_datasets,\n",
    "    eval_dataset=validation_datasets,\n",
    "    # data_collator=data_collator, # for padding, optional here\n",
    "    tokenizer=pretrained_bert_tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()\n",
    "\n",
    "\n",
    "# solution to \"AttributeError: module 'torch.distributed' has no attribute 'is_initialized'\"\n",
    "# https://stackoverflow.com/questions/72641886/attributeerror-module-torch-distributed-has-no-attribute-is-initialized-in/72641887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eb44079e-9548-4c39-a844-f5d2ef13c88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "torch.distributed process group is initialized, but local_rank == -1. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "/Users/sean.huang/miniconda3/envs/m1_new/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1600\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1300' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1300/1600 11:54 < 02:45, 1.82 it/s, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.190032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.168284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.127641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.129792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.149050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.124293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.102385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.097123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.099075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.108774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.120638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.120108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./rest_ner_model/checkpoint-500\n",
      "Configuration saved in ./rest_ner_model/checkpoint-500/config.json\n",
      "Model weights saved in ./rest_ner_model/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./rest_ner_model/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./rest_ner_model/checkpoint-500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./rest_ner_model/checkpoint-1000\n",
      "Configuration saved in ./rest_ner_model/checkpoint-1000/config.json\n",
      "Model weights saved in ./rest_ner_model/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./rest_ner_model/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./rest_ner_model/checkpoint-1000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./rest_ner_model/checkpoint-1000 (score: 0.1087740883231163).\n",
      "Saving model checkpoint to ./rest_ner_model\n",
      "Configuration saved in ./rest_ner_model/config.json\n",
      "Model weights saved in ./rest_ner_model/pytorch_model.bin\n",
      "tokenizer config file saved in ./rest_ner_model/tokenizer_config.json\n",
      "Special tokens file saved in ./rest_ner_model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./rest_ner_model',\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=10,\n",
    "    logging_strategy='steps',\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps = 100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=eval_loss,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(pretrained_bert_tokenizer)\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model=pretrained_bert_model,\n",
    "    args=args,\n",
    "    train_dataset=train_datasets,\n",
    "    eval_dataset=validation_datasets,\n",
    "    # data_collator=data_collator, # for padding, optional here\n",
    "    tokenizer=pretrained_bert_tokenizer,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3937f7d5-10da-4790-a206-f727d90ce401",
   "metadata": {},
   "source": [
    "### Load finetune Bert model for Rest NER task, and do prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01d6bb37-a67a-407e-b523-2a3c5ee69322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "finetuned_rest_tokenizer = AutoTokenizer.from_pretrained('./rest_ner_model')\n",
    "finetuned_rest_model = AutoModelForTokenClassification.from_pretrained('./rest_ner_model', num_labels=len(tag2id.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da2e3ecd-5682-4d60-910e-bb901ac2134f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'entity': 'B', 'score': 0.9999516, 'index': 4, 'word': 'food', 'start': 19, 'end': 23}, {'entity': 'B', 'score': 0.999948, 'index': 7, 'word': 'environment', 'start': 30, 'end': 41}], [{'entity': 'B', 'score': 0.999959, 'index': 7, 'word': 'view', 'start': 16, 'end': 20}]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "rest_ner_pipeline = pipeline('ner', model=finetuned_rest_model, tokenizer=finetuned_rest_tokenizer)\n",
    "\n",
    "examples = ['Serves really good food, nice environment', 'Oh yeah ... the view was good too']\n",
    "rest_ner_results = rest_ner_pipeline(examples)\n",
    "print(rest_ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d902ac-a66d-4a9d-8646-545f01a6b180",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "759e2682-8eb9-41d1-872a-868743eba8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_ner_test_file = '/Users/sean.huang/Documents/my/books and courses/ztgg/Shared Code/BERT-for-ABSA/ae/rest/test.json'\n",
    "rest_ner_test_samples = json.load(open(rest_ner_test_file))\n",
    "len(rest_ner_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0798adf0-fbff-4447-96a0-559ed40ae82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rest_ner_test_samples = {'sentence_id':[], 'sentence':[], 'full_sentence':[], 'label':[]}\n",
    "\n",
    "for key in rest_ner_test_samples.keys():\n",
    "    dict_rest_ner_test_samples['sentence_id'].append(key)\n",
    "    dict_rest_ner_test_samples['sentence'].append(rest_ner_test_samples[key]['sentence'])\n",
    "    dict_rest_ner_test_samples['full_sentence'].append(' '.join(rest_ner_test_samples[key]['sentence']))\n",
    "    dict_rest_ner_test_samples['label'].append(rest_ner_test_samples[key]['label'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cc53615-d052-411d-9b2d-e9aa17c91bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_ner_results = rest_ner_pipeline(dict_rest_ner_test_samples['full_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ebb84b2-a39e-44af-b4de-f03a06dff892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [{'entity': 'B',\n",
       "   'score': 0.9999312,\n",
       "   'index': 4,\n",
       "   'word': 'su',\n",
       "   'start': 19,\n",
       "   'end': 21},\n",
       "  {'entity': 'B',\n",
       "   'score': 0.9999331,\n",
       "   'index': 5,\n",
       "   'word': '##shi',\n",
       "   'start': 21,\n",
       "   'end': 24}],\n",
       " [{'entity': 'B',\n",
       "   'score': 0.99994266,\n",
       "   'index': 4,\n",
       "   'word': 'portions',\n",
       "   'start': 16,\n",
       "   'end': 24}],\n",
       " [{'entity': 'B',\n",
       "   'score': 0.9995384,\n",
       "   'index': 1,\n",
       "   'word': 'green',\n",
       "   'start': 0,\n",
       "   'end': 5},\n",
       "  {'entity': 'I',\n",
       "   'score': 0.99952495,\n",
       "   'index': 2,\n",
       "   'word': 'tea',\n",
       "   'start': 6,\n",
       "   'end': 9},\n",
       "  {'entity': 'I',\n",
       "   'score': 0.999559,\n",
       "   'index': 3,\n",
       "   'word': 'cr',\n",
       "   'start': 10,\n",
       "   'end': 12},\n",
       "  {'entity': 'I',\n",
       "   'score': 0.99959844,\n",
       "   'index': 4,\n",
       "   'word': '##eme',\n",
       "   'start': 12,\n",
       "   'end': 15},\n",
       "  {'entity': 'I',\n",
       "   'score': 0.999647,\n",
       "   'index': 5,\n",
       "   'word': 'br',\n",
       "   'start': 16,\n",
       "   'end': 18},\n",
       "  {'entity': 'I',\n",
       "   'score': 0.99965715,\n",
       "   'index': 6,\n",
       "   'word': '##ule',\n",
       "   'start': 18,\n",
       "   'end': 21},\n",
       "  {'entity': 'I',\n",
       "   'score': 0.9996382,\n",
       "   'index': 7,\n",
       "   'word': '##e',\n",
       "   'start': 21,\n",
       "   'end': 22}],\n",
       " []]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_ner_results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6825e60-e025-4fb9-98de-b03eee91b282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{},\n",
       " {'sushi': 'B'},\n",
       " {'portions': 'B'},\n",
       " {'green': 'B', 'tea': 'I', 'creme': 'I', 'brulee': 'I'},\n",
       " {}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reformatted_rest_ner_results = []\n",
    "\n",
    "for sentence_result in rest_ner_results:\n",
    "    if len(sentence_result) == 0:\n",
    "        reformatted_rest_ner_results.append({})\n",
    "        continue\n",
    "    last_label = sentence_result[0]['entity']\n",
    "    last_token = sentence_result[0]['word']\n",
    "    reformatted_sent_result = {}\n",
    "    for token_result in sentence_result[1:]:\n",
    "        label = token_result['entity']\n",
    "        token = token_result['word']\n",
    "        \n",
    "        if token.startswith('##') is False:\n",
    "            if last_label != '':\n",
    "                reformatted_sent_result[last_token] = last_label\n",
    "            last_token = token\n",
    "            last_label = label\n",
    "        else:\n",
    "            last_token = last_token + token[2:] # remove '##' \n",
    "            last_label = label            \n",
    "\n",
    "    if last_label != '':\n",
    "        reformatted_sent_result[last_token] = last_label\n",
    "    \n",
    "    reformatted_rest_ner_results.append(reformatted_sent_result)\n",
    "    \n",
    "reformatted_rest_ner_results[:5]         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1505ab7c-e1e2-4fca-a4cd-48dfc3a9d570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = []\n",
    "\n",
    "for tokens, ner_tags in zip(dict_rest_ner_test_samples['sentence'], reformatted_rest_ner_results):\n",
    "    \n",
    "    pred = []\n",
    "    for token in tokens:\n",
    "        if token not in ner_tags.keys():\n",
    "            pred.append('O')\n",
    "            y_pred.append('O')\n",
    "        else:\n",
    "            pred.append(ner_tags[token])\n",
    "            y_pred.append(ner_tags[token])\n",
    "    # print(tokens, ner_tags, pred)\n",
    "    \n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63fbc632-4a15-4f77-ab09-aabd77bf63d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10096"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b74c09b1-d83d-4450-bd7c-01882348780e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10096"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = []\n",
    "for each in dict_rest_ner_test_samples['label']:\n",
    "    y_true.extend(each)\n",
    "\n",
    "len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "198401eb-d48b-4939-bf03-af6118f376d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9539421553090333"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91365cc9-27ba-4fe6-839f-a26d698e1e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.79467681, 0.73205742, 0.96784532]),\n",
       " array([0.68300654, 0.54642857, 0.98435463]),\n",
       " array([0.73462214, 0.62576687, 0.97603016]),\n",
       " array([ 612,  280, 9204]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_recall_fscore_support(y_true, y_pred, labels=['B', 'I', 'O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "838f679c-1220-4434-85d7-cd4ebe8a9a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 418,    9,  185],\n",
       "       [  11,  153,  116],\n",
       "       [  97,   47, 9060]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_true, y_pred, labels=['B', 'I', 'O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "68129bab-3994-4d9e-8753-c4a0d2eda5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bab5371e-97fd-4884-82ef-89f322ce6cd7",
   "metadata": {},
   "source": [
    "## Home Work: do laptop NER (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a0dc7-dd22-47d9-821a-f5c14c69086e",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9646770-569c-4346-8393-66f785c38ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
