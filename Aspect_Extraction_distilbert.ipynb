{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1276ed9-da18-41b4-af09-c4907de76c46",
   "metadata": {},
   "source": [
    "## Restaurant NER (AE - Aspect Extraction) By Distilbert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec683a-f5bc-4e3f-8c6d-b685bf043a50",
   "metadata": {},
   "source": [
    "### Preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bfe08b4-aa9c-426d-b68b-db02399e312d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1850"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "rest_ner_training_file = '/Users/sean.huang/Documents/my/books and courses/ztgg/Shared Code/BERT-for-ABSA/ae/rest/train.json'\n",
    "rest_ner_training_samples = json.load(open(rest_ner_training_file))\n",
    "len(rest_ner_training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2073e5c4-4b22-4b49-9d6f-4d9f1e7e97d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': ['O', 'O', 'O', 'B'], 'sentence': ['I', 'LOVE', 'their', 'Thai']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_ner_training_samples['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bda66adf-796f-4ef1-94d7-eee6362023fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[I, LOVE, their, Thai]</td>\n",
       "      <td>[O, O, O, B]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[The, service, was, attentive, .]</td>\n",
       "      <td>[O, B, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[I, go, twice, a, month, !]</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[The, food, was, average, to, above, -, averag...</td>\n",
       "      <td>[O, B, O, O, O, O, O, O, O, O, B, I, I, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Ask, for, Usha, ,, the, nicest, bartender, in...</td>\n",
       "      <td>[O, O, B, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845</th>\n",
       "      <td>1845</td>\n",
       "      <td>[You, can, get, a, table, without, a, reservat...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>1846</td>\n",
       "      <td>[I, have, eaten, at, some, of, the, 'best, ', ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>1847</td>\n",
       "      <td>[Worth, visiting, the, 1st, Ave, spot, because...</td>\n",
       "      <td>[O, O, O, B, I, I, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>1848</td>\n",
       "      <td>[Its, located, in, greenewich, village, .]</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>1849</td>\n",
       "      <td>[Overall, the, food, quality, was, pretty, goo...</td>\n",
       "      <td>[O, O, B, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1850 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_id                                           sentence  \\\n",
       "0              0                             [I, LOVE, their, Thai]   \n",
       "1              1                  [The, service, was, attentive, .]   \n",
       "2              2                        [I, go, twice, a, month, !]   \n",
       "3              3  [The, food, was, average, to, above, -, averag...   \n",
       "4              4  [Ask, for, Usha, ,, the, nicest, bartender, in...   \n",
       "...          ...                                                ...   \n",
       "1845        1845  [You, can, get, a, table, without, a, reservat...   \n",
       "1846        1846  [I, have, eaten, at, some, of, the, 'best, ', ...   \n",
       "1847        1847  [Worth, visiting, the, 1st, Ave, spot, because...   \n",
       "1848        1848         [Its, located, in, greenewich, village, .]   \n",
       "1849        1849  [Overall, the, food, quality, was, pretty, goo...   \n",
       "\n",
       "                                                  label  \n",
       "0                                          [O, O, O, B]  \n",
       "1                                       [O, B, O, O, O]  \n",
       "2                                    [O, O, O, O, O, O]  \n",
       "3     [O, B, O, O, O, O, O, O, O, O, B, I, I, O, O, ...  \n",
       "4                        [O, O, B, O, O, O, O, O, O, O]  \n",
       "...                                                 ...  \n",
       "1845  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1846  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1847            [O, O, O, B, I, I, O, O, O, O, O, O, O]  \n",
       "1848                                 [O, O, O, O, O, O]  \n",
       "1849  [O, O, B, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "\n",
       "[1850 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_rest_ner_training_samples = {'sentence_id':[], 'sentence':[], 'label':[]}\n",
    "\n",
    "for key in rest_ner_training_samples.keys():\n",
    "    dict_rest_ner_training_samples['sentence_id'].append(key)\n",
    "    dict_rest_ner_training_samples['sentence'].append(rest_ner_training_samples[key]['sentence'])\n",
    "    dict_rest_ner_training_samples['label'].append(rest_ner_training_samples[key]['label'])\n",
    "    \n",
    "dict_rest_ner_training_samples\n",
    "\n",
    "df_rest_ner_training_samples = pd.DataFrame.from_dict(dict_rest_ner_training_samples)\n",
    "df_rest_ner_training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42733d52-10b5-4cbc-b14a-e87be666f8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>LOVE</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>their</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thai</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>1849</td>\n",
       "      <td>front</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>1849</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>1849</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>1849</td>\n",
       "      <td>guest</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>1849</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26957 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence_id sentence label\n",
       "0              0        I     O\n",
       "0              0     LOVE     O\n",
       "0              0    their     O\n",
       "0              0     Thai     B\n",
       "1              1      The     O\n",
       "...          ...      ...   ...\n",
       "1849        1849    front     O\n",
       "1849        1849       of     O\n",
       "1849        1849      the     O\n",
       "1849        1849    guest     O\n",
       "1849        1849        .     O\n",
       "\n",
       "[26957 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rest_ner_training_samples_exploded = df_rest_ner_training_samples.explode(['sentence', 'label'])\n",
    "df_rest_ner_training_samples_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6fb3a6a-5f5c-47ec-b66b-1462e191a0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Murray', 0), ('sure', 1), ('short', 2), ('watching', 3), ('outside', 4)] [(0, 'Murray'), (1, 'sure'), (2, 'short'), (3, 'watching'), (4, 'outside')]\n",
      "{'I': 0, 'O': 1, 'B': 2} {0: 'I', 1: 'O', 2: 'B'}\n"
     ]
    }
   ],
   "source": [
    "def get_mapping(data, column):\n",
    "    t2id = {}\n",
    "    id2t = {}\n",
    " \n",
    "    vocab = list(set(data[column].to_list()))\n",
    "    \n",
    "    id2t = {i:t for i, t in enumerate(vocab)}\n",
    "    t2id = {t:i for  i, t in enumerate(vocab)}\n",
    "    return t2id, id2t\n",
    "\n",
    "\n",
    "token2id, id2token = get_mapping(df_rest_ner_training_samples_exploded, 'sentence')\n",
    "tag2id, id2tag = get_mapping(df_rest_ner_training_samples_exploded, 'label')\n",
    "\n",
    "print(list(token2id.items())[0:5], list(id2token.items())[0:5])\n",
    "print(tag2id, id2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30140871-d211-4a4d-bc46-176a01dada7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>word_id</th>\n",
       "      <th>tag_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>O</td>\n",
       "      <td>468</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>LOVE</td>\n",
       "      <td>O</td>\n",
       "      <td>2687</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>their</td>\n",
       "      <td>O</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thai</td>\n",
       "      <td>B</td>\n",
       "      <td>2041</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "      <td>2709</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>service</td>\n",
       "      <td>B</td>\n",
       "      <td>2500</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>was</td>\n",
       "      <td>O</td>\n",
       "      <td>341</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>attentive</td>\n",
       "      <td>O</td>\n",
       "      <td>2372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>2587</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I</td>\n",
       "      <td>O</td>\n",
       "      <td>468</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>go</td>\n",
       "      <td>O</td>\n",
       "      <td>1319</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>twice</td>\n",
       "      <td>O</td>\n",
       "      <td>2413</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "      <td>1800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>month</td>\n",
       "      <td>O</td>\n",
       "      <td>426</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>!</td>\n",
       "      <td>O</td>\n",
       "      <td>440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "      <td>2709</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>food</td>\n",
       "      <td>B</td>\n",
       "      <td>779</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>was</td>\n",
       "      <td>O</td>\n",
       "      <td>341</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>average</td>\n",
       "      <td>O</td>\n",
       "      <td>1141</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "      <td>1483</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_id   sentence label  word_id  tag_id\n",
       "0           0          I     O      468       1\n",
       "0           0       LOVE     O     2687       1\n",
       "0           0      their     O       22       1\n",
       "0           0       Thai     B     2041       2\n",
       "1           1        The     O     2709       1\n",
       "1           1    service     B     2500       2\n",
       "1           1        was     O      341       1\n",
       "1           1  attentive     O     2372       1\n",
       "1           1          .     O     2587       1\n",
       "2           2          I     O      468       1\n",
       "2           2         go     O     1319       1\n",
       "2           2      twice     O     2413       1\n",
       "2           2          a     O     1800       1\n",
       "2           2      month     O      426       1\n",
       "2           2          !     O      440       1\n",
       "3           3        The     O     2709       1\n",
       "3           3       food     B      779       2\n",
       "3           3        was     O      341       1\n",
       "3           3    average     O     1141       1\n",
       "3           3         to     O     1483       1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rest_ner_training_samples_exploded['word_id'] = df_rest_ner_training_samples_exploded['sentence'].map(token2id)\n",
    "df_rest_ner_training_samples_exploded['tag_id'] = df_rest_ner_training_samples_exploded['label'].map(tag2id)\n",
    "df_rest_ner_training_samples_exploded.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ba851be-18b6-483d-bd1d-b9bad715c101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_id    0\n",
       "sentence       0\n",
       "label          0\n",
       "word_id        0\n",
       "tag_id         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rest_ner_training_samples_exploded.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63831610-a097-44a3-a406-d3d2bd9487a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/57/qcnnk6sn1vvbtwg1g1xc6kmw0000gp/T/ipykernel_92509/3037838837.py:1: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  df_rest_ner_training_samples = df_rest_ner_training_samples_exploded.groupby(['sentence_id'])['sentence', 'label', 'word_id', 'tag_id'].agg(lambda x: list(x))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>word_id</th>\n",
       "      <th>tag_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[I, LOVE, their, Thai]</td>\n",
       "      <td>[O, O, O, B]</td>\n",
       "      <td>[468, 2687, 22, 2041]</td>\n",
       "      <td>[1, 1, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[The, service, was, attentive, .]</td>\n",
       "      <td>[O, B, O, O, O]</td>\n",
       "      <td>[2709, 2500, 341, 2372, 2587]</td>\n",
       "      <td>[1, 2, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[Everything, is, always, cooked, to, perfectio...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B, O, O, O, O, B, O, ...</td>\n",
       "      <td>[3386, 810, 3419, 2207, 1483, 3748, 2449, 1646...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>[good, music, ,, great, food, ,, speedy, servi...</td>\n",
       "      <td>[O, B, O, O, B, O, O, B, O, O, O]</td>\n",
       "      <td>[1710, 2490, 2449, 306, 779, 2449, 3515, 2500,...</td>\n",
       "      <td>[1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>[But, the, pizza, is, way, to, expensive, .]</td>\n",
       "      <td>[O, O, B, O, O, O, O, O]</td>\n",
       "      <td>[107, 1646, 3749, 810, 1148, 1483, 1478, 2587]</td>\n",
       "      <td>[1, 1, 2, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>[The, only, beverage, we, did, receive, was, w...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[2709, 1919, 1970, 555, 1721, 1588, 341, 3635,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>[A, friend, from, Seattle, and, I, went, on, a...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[3487, 395, 480, 2256, 3407, 468, 1812, 1746, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>[Went, on, a, 3, day, oyster, binge, ,, with, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B, O, O, O, O, O, ...</td>\n",
       "      <td>[2893, 1746, 1800, 2269, 125, 3123, 2036, 2449...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>[Service, is, fast, and, friendly, .]</td>\n",
       "      <td>[B, O, O, O, O, O]</td>\n",
       "      <td>[3247, 810, 2695, 3407, 3214, 2587]</td>\n",
       "      <td>[2, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>[I, ordered, the, vitello, alla, marsala, and,...</td>\n",
       "      <td>[O, O, O, B, I, I, O, O, O, O, O, O]</td>\n",
       "      <td>[468, 68, 1646, 2915, 2263, 3008, 3407, 468, 3...</td>\n",
       "      <td>[1, 1, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1850 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      sentence  \\\n",
       "sentence_id                                                      \n",
       "0                                       [I, LOVE, their, Thai]   \n",
       "1                            [The, service, was, attentive, .]   \n",
       "10           [Everything, is, always, cooked, to, perfectio...   \n",
       "100          [good, music, ,, great, food, ,, speedy, servi...   \n",
       "1000              [But, the, pizza, is, way, to, expensive, .]   \n",
       "...                                                        ...   \n",
       "995          [The, only, beverage, we, did, receive, was, w...   \n",
       "996          [A, friend, from, Seattle, and, I, went, on, a...   \n",
       "997          [Went, on, a, 3, day, oyster, binge, ,, with, ...   \n",
       "998                      [Service, is, fast, and, friendly, .]   \n",
       "999          [I, ordered, the, vitello, alla, marsala, and,...   \n",
       "\n",
       "                                                         label  \\\n",
       "sentence_id                                                      \n",
       "0                                                 [O, O, O, B]   \n",
       "1                                              [O, B, O, O, O]   \n",
       "10           [O, O, O, O, O, O, O, O, B, O, O, O, O, B, O, ...   \n",
       "100                          [O, B, O, O, B, O, O, B, O, O, O]   \n",
       "1000                                  [O, O, B, O, O, O, O, O]   \n",
       "...                                                        ...   \n",
       "995                       [O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "996                 [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "997          [O, O, O, O, O, O, O, O, O, B, O, O, O, O, O, ...   \n",
       "998                                         [B, O, O, O, O, O]   \n",
       "999                       [O, O, O, B, I, I, O, O, O, O, O, O]   \n",
       "\n",
       "                                                       word_id  \\\n",
       "sentence_id                                                      \n",
       "0                                        [468, 2687, 22, 2041]   \n",
       "1                                [2709, 2500, 341, 2372, 2587]   \n",
       "10           [3386, 810, 3419, 2207, 1483, 3748, 2449, 1646...   \n",
       "100          [1710, 2490, 2449, 306, 779, 2449, 3515, 2500,...   \n",
       "1000            [107, 1646, 3749, 810, 1148, 1483, 1478, 2587]   \n",
       "...                                                        ...   \n",
       "995          [2709, 1919, 1970, 555, 1721, 1588, 341, 3635,...   \n",
       "996          [3487, 395, 480, 2256, 3407, 468, 1812, 1746, ...   \n",
       "997          [2893, 1746, 1800, 2269, 125, 3123, 2036, 2449...   \n",
       "998                        [3247, 810, 2695, 3407, 3214, 2587]   \n",
       "999          [468, 68, 1646, 2915, 2263, 3008, 3407, 468, 3...   \n",
       "\n",
       "                                                        tag_id  \n",
       "sentence_id                                                     \n",
       "0                                                 [1, 1, 1, 2]  \n",
       "1                                              [1, 2, 1, 1, 1]  \n",
       "10           [1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, ...  \n",
       "100                          [1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1]  \n",
       "1000                                  [1, 1, 2, 1, 1, 1, 1, 1]  \n",
       "...                                                        ...  \n",
       "995                       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "996                 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  \n",
       "997          [1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, ...  \n",
       "998                                         [2, 1, 1, 1, 1, 1]  \n",
       "999                       [1, 1, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1]  \n",
       "\n",
       "[1850 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rest_ner_training_samples = df_rest_ner_training_samples_exploded.groupby(['sentence_id'])['sentence', 'label', 'word_id', 'tag_id'].agg(lambda x: list(x))\n",
    "df_rest_ner_training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "655e98ff-c8be-4416-a938-35364af27bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sent_length = max([len(each) for each in list(df_rest_ner_training_samples['sentence'])])\n",
    "max_sent_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e27014-3682-440e-bf76-9e896a1f84ff",
   "metadata": {},
   "source": [
    "### Load and finetune pretrained Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc29cb3-82c3-4696-ac4d-f52ef0e62954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "pretrained_bert_model_name = 'distilbert-base-uncased'\n",
    "pretrained_bert_tokenizer = AutoTokenizer.from_pretrained(pretrained_bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0110e2fa-2201-45d7-ad8b-284ff3e802fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 2293, 2037, 7273, 102], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pretrained_bert_tokenizer(list(df_rest_ner_training_samples['sentence'])[0], is_split_into_words=True)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9ac7c1f-b963-49e2-a47b-09405f4bfc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function align_ner_labels at 0x12fb3eaf0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a04c28acef74af984b2e592ee44179f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['O', 'O', 'O', 'B']\n",
      "[None, 0, 1, 2, 3, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O']\n",
      "[None, 0, 1, 2, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'B', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 2, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 16, 16, 17, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 2, 2, 3, 4, 5, 6, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'B', 'O', 'O', 'O']\n",
      "[None, 0, 1, 1, 2, 3, 4, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 2, 2, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 12, 13, 14, 15, 16, 17, 18, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 4, 4, 5, 6, 7, 8, 9, 10, 11, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a814856788874f1ca74ef0c34368ad71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 0, 1, 2, 3, 4, 4, 5, 6, 7, 8, 9, 9, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'B', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 2, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['B', 'O', 'O', 'O', 'B', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 1, 1, 1, 2, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 19, 19, 20, 21, 22, 23, 24, 25, 26, 26, 27, 28, 29, 30, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 3, 3, 4, 5, 6, 6, 7, 8, 9, 10, 11, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 1, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 19, 19, 20, 21, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 0, 0, 1, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['B', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 4, 4, 5, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 2, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 2, 0, 0, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 9, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 0, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['B', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 2, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 36, 36, 37, 37, 37, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 12, 13, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O']\n",
      "[None, 0, 1, 2, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 7, 7, 8, 8, 8, 9, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "0\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
      "[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def align_ner_labels(samples):\n",
    "    inputs = pretrained_bert_tokenizer(samples['sentence'], truncation=True, padding='max_length', max_length=128, is_split_into_words=True)\n",
    "\n",
    "    label_input = []\n",
    "    for i, label in enumerate(samples['label']):\n",
    "        word_ids = inputs.word_ids(batch_index=i)\n",
    "        \n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                label_ids.append(tag2id[label[word_idx]])\n",
    "        \n",
    "        if i == 0:\n",
    "            print(i)\n",
    "            print(label)\n",
    "            print(word_ids)\n",
    "            print(label_ids)\n",
    "            \n",
    "        label_input.append(label_ids)\n",
    "        \n",
    "    inputs['labels'] = label_input\n",
    "    \n",
    "    # print('inputs', inputs.keys())\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "train_datasets = Dataset.from_pandas(df_rest_ner_training_samples[:1600]).map(align_ner_labels, batched=True, batch_size=100).remove_columns(['sentence', 'label', 'word_id', 'tag_id', 'sentence_id'])\n",
    "validation_datasets = Dataset.from_pandas(df_rest_ner_training_samples[1600:]).map(align_ner_labels, batched=True, batch_size=10).remove_columns(['sentence', 'label', 'word_id', 'tag_id', 'sentence_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16dc08e7-ae33-420c-afbe-4a961de008bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1600\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "628c7f18-5372-417c-a738-44274af40711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 250\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03af1e33-66ea-4513-b255-09bcb6724652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')   # mps is for Apple M1 GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "480076ef-b772-4d1b-8bfa-61d2a108ad7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForTokenClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import torch\n",
    "\n",
    "pretrained_bert_model = AutoModelForTokenClassification.from_pretrained(pretrained_bert_model_name, num_labels=len(tag2id.keys()), id2label=id2tag, label2id=tag2id)\n",
    "\n",
    "pretrained_bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d27c9321-93b2-44c6-8fc8-5612dad8e15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torch.distributed process group is initialized, but local_rank == -1. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "/Users/sean.huang/miniconda3/envs/m1_new/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1600\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1600/1600 17:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.195400</td>\n",
       "      <td>0.132977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.072600</td>\n",
       "      <td>0.139341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.181322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.207930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.246490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.275901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.268061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.282526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.274736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.278625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./rest_ner_model/checkpoint-500\n",
      "Configuration saved in ./rest_ner_model/checkpoint-500/config.json\n",
      "Model weights saved in ./rest_ner_model/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./rest_ner_model/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./rest_ner_model/checkpoint-500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./rest_ner_model/checkpoint-1000\n",
      "Configuration saved in ./rest_ner_model/checkpoint-1000/config.json\n",
      "Model weights saved in ./rest_ner_model/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./rest_ner_model/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./rest_ner_model/checkpoint-1000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./rest_ner_model/checkpoint-1500\n",
      "Configuration saved in ./rest_ner_model/checkpoint-1500/config.json\n",
      "Model weights saved in ./rest_ner_model/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./rest_ner_model/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./rest_ner_model/checkpoint-1500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./rest_ner_model\n",
      "Configuration saved in ./rest_ner_model/config.json\n",
      "Model weights saved in ./rest_ner_model/pytorch_model.bin\n",
      "tokenizer config file saved in ./rest_ner_model/tokenizer_config.json\n",
      "Special tokens file saved in ./rest_ner_model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='./rest_ner_model_distilbert',\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=10,\n",
    "    logging_strategy='epoch',\n",
    "    evaluation_strategy='epoch',\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(pretrained_bert_tokenizer)\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model=pretrained_bert_model,\n",
    "    args=args,\n",
    "    train_dataset=train_datasets,\n",
    "    eval_dataset=validation_datasets,\n",
    "    # data_collator=data_collator, # for padding, optional here\n",
    "    tokenizer=pretrained_bert_tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()\n",
    "\n",
    "\n",
    "# solution to \"AttributeError: module 'torch.distributed' has no attribute 'is_initialized'\"\n",
    "# https://stackoverflow.com/questions/72641886/attributeerror-module-torch-distributed-has-no-attribute-is-initialized-in/72641887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb44079e-9548-4c39-a844-f5d2ef13c88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "torch.distributed process group is initialized, but local_rank == -1. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "/Users/sean.huang/miniconda3/envs/m1_new/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1600\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1600\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 800/1600 13:02 < 13:04, 1.02 it/s, Epoch 5/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.352193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.285597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.294611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.297299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.304166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.321328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.307595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.324975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./rest_ner_model_distilbert/checkpoint-500\n",
      "Configuration saved in ./rest_ner_model_distilbert/checkpoint-500/config.json\n",
      "Model weights saved in ./rest_ner_model_distilbert/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./rest_ner_model_distilbert/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./rest_ner_model_distilbert/checkpoint-500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 250\n",
      "  Batch size = 2\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./rest_ner_model_distilbert/checkpoint-500 (score: 0.30416595935821533).\n",
      "Saving model checkpoint to ./rest_ner_model_distilbert\n",
      "Configuration saved in ./rest_ner_model_distilbert/config.json\n",
      "Model weights saved in ./rest_ner_model_distilbert/pytorch_model.bin\n",
      "tokenizer config file saved in ./rest_ner_model_distilbert/tokenizer_config.json\n",
      "Special tokens file saved in ./rest_ner_model_distilbert/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./rest_ner_model_distilbert',\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=10,\n",
    "    logging_strategy='steps',\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps = 100,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(pretrained_bert_tokenizer)\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model=pretrained_bert_model,\n",
    "    args=args,\n",
    "    train_dataset=train_datasets,\n",
    "    eval_dataset=validation_datasets,\n",
    "    # data_collator=data_collator, # for padding, optional here\n",
    "    tokenizer=pretrained_bert_tokenizer,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3937f7d5-10da-4790-a206-f727d90ce401",
   "metadata": {},
   "source": [
    "### Load finetune Bert model for Rest NER task, and do prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01d6bb37-a67a-407e-b523-2a3c5ee69322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./rest_ner_model_distilbert/added_tokens.json. We won't load it.\n",
      "loading file ./rest_ner_model_distilbert/vocab.txt\n",
      "loading file ./rest_ner_model_distilbert/tokenizer.json\n",
      "loading file None\n",
      "loading file ./rest_ner_model_distilbert/special_tokens_map.json\n",
      "loading file ./rest_ner_model_distilbert/tokenizer_config.json\n",
      "loading configuration file ./rest_ner_model_distilbert/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"./rest_ner_model_distilbert\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"I\",\n",
      "    \"1\": \"O\",\n",
      "    \"2\": \"B\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"B\": 2,\n",
      "    \"I\": 0,\n",
      "    \"O\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./rest_ner_model_distilbert/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForTokenClassification.\n",
      "\n",
      "All the weights of DistilBertForTokenClassification were initialized from the model checkpoint at ./rest_ner_model_distilbert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForTokenClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "finetuned_rest_tokenizer = AutoTokenizer.from_pretrained('./rest_ner_model_distilbert')\n",
    "finetuned_rest_model = AutoModelForTokenClassification.from_pretrained('./rest_ner_model_distilbert', num_labels=len(tag2id.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da2e3ecd-5682-4d60-910e-bb901ac2134f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'entity': 'B', 'score': 0.99975616, 'index': 4, 'word': 'food', 'start': 19, 'end': 23}, {'entity': 'B', 'score': 0.99974054, 'index': 7, 'word': 'environment', 'start': 30, 'end': 41}], [{'entity': 'B', 'score': 0.999783, 'index': 7, 'word': 'view', 'start': 16, 'end': 20}]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "rest_ner_pipeline = pipeline('ner', model=finetuned_rest_model, tokenizer=finetuned_rest_tokenizer)\n",
    "\n",
    "examples = ['Serves really good food, nice environment', 'Oh yeah ... the view was good too']\n",
    "rest_ner_results = rest_ner_pipeline(examples)\n",
    "print(rest_ner_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d902ac-a66d-4a9d-8646-545f01a6b180",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "759e2682-8eb9-41d1-872a-868743eba8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "676"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_ner_test_file = '/Users/sean.huang/Documents/my/books and courses/ztgg/Shared Code/BERT-for-ABSA/ae/rest/test.json'\n",
    "rest_ner_test_samples = json.load(open(rest_ner_test_file))\n",
    "len(rest_ner_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0798adf0-fbff-4447-96a0-559ed40ae82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rest_ner_test_samples = {'sentence_id':[], 'sentence':[], 'full_sentence':[], 'label':[]}\n",
    "\n",
    "for key in rest_ner_test_samples.keys():\n",
    "    dict_rest_ner_test_samples['sentence_id'].append(key)\n",
    "    dict_rest_ner_test_samples['sentence'].append(rest_ner_test_samples[key]['sentence'])\n",
    "    dict_rest_ner_test_samples['full_sentence'].append(' '.join(rest_ner_test_samples[key]['sentence']))\n",
    "    dict_rest_ner_test_samples['label'].append(rest_ner_test_samples[key]['label'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8cc53615-d052-411d-9b2d-e9aa17c91bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_ner_results = rest_ner_pipeline(dict_rest_ner_test_samples['full_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ebb84b2-a39e-44af-b4de-f03a06dff892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [{'entity': 'B',\n",
       "   'score': 0.9998142,\n",
       "   'index': 4,\n",
       "   'word': 'su',\n",
       "   'start': 19,\n",
       "   'end': 21},\n",
       "  {'entity': 'B',\n",
       "   'score': 0.99978334,\n",
       "   'index': 5,\n",
       "   'word': '##shi',\n",
       "   'start': 21,\n",
       "   'end': 24}],\n",
       " [{'entity': 'B',\n",
       "   'score': 0.9990421,\n",
       "   'index': 4,\n",
       "   'word': 'portions',\n",
       "   'start': 16,\n",
       "   'end': 24}],\n",
       " [{'entity': 'B',\n",
       "   'score': 0.9991598,\n",
       "   'index': 1,\n",
       "   'word': 'green',\n",
       "   'start': 0,\n",
       "   'end': 5},\n",
       "  {'entity': 'I',\n",
       "   'score': 0.9997466,\n",
       "   'index': 2,\n",
       "   'word': 'tea',\n",
       "   'start': 6,\n",
       "   'end': 9},\n",
       "  {'entity': 'I',\n",
       "   'score': 0.9999201,\n",
       "   'index': 3,\n",
       "   'word': 'cr',\n",
       "   'start': 10,\n",
       "   'end': 12},\n",
       "  {'entity': 'I',\n",
       "   'score': 0.9999298,\n",
       "   'index': 4,\n",
       "   'word': '##eme',\n",
       "   'start': 12,\n",
       "   'end': 15},\n",
       "  {'entity': 'I',\n",
       "   'score': 0.99991655,\n",
       "   'index': 5,\n",
       "   'word': 'br',\n",
       "   'start': 16,\n",
       "   'end': 18},\n",
       "  {'entity': 'I',\n",
       "   'score': 0.99991536,\n",
       "   'index': 6,\n",
       "   'word': '##ule',\n",
       "   'start': 18,\n",
       "   'end': 21},\n",
       "  {'entity': 'I',\n",
       "   'score': 0.99991417,\n",
       "   'index': 7,\n",
       "   'word': '##e',\n",
       "   'start': 21,\n",
       "   'end': 22}],\n",
       " [{'entity': 'B',\n",
       "   'score': 0.99712116,\n",
       "   'index': 7,\n",
       "   'word': 'restaurant',\n",
       "   'start': 17,\n",
       "   'end': 27}]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_ner_results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6825e60-e025-4fb9-98de-b03eee91b282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{},\n",
       " {'sushi': 'B'},\n",
       " {'portions': 'B'},\n",
       " {'green': 'B', 'tea': 'I', 'creme': 'I', 'brulee': 'I'},\n",
       " {'restaurant': 'B'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reformatted_rest_ner_results = []\n",
    "\n",
    "for sentence_result in rest_ner_results:\n",
    "    if len(sentence_result) == 0:\n",
    "        reformatted_rest_ner_results.append({})\n",
    "        continue\n",
    "    last_label = sentence_result[0]['entity']\n",
    "    last_token = sentence_result[0]['word']\n",
    "    reformatted_sent_result = {}\n",
    "    for token_result in sentence_result[1:]:\n",
    "        label = token_result['entity']\n",
    "        token = token_result['word']\n",
    "        \n",
    "        if token.startswith('##') is False:\n",
    "            if last_label != '':\n",
    "                reformatted_sent_result[last_token] = last_label\n",
    "            last_token = token\n",
    "            last_label = label\n",
    "        else:\n",
    "            last_token = last_token + token[2:] # remove '##' \n",
    "            last_label = label            \n",
    "\n",
    "    if last_label != '':\n",
    "        reformatted_sent_result[last_token] = last_label\n",
    "    \n",
    "    reformatted_rest_ner_results.append(reformatted_sent_result)\n",
    "    \n",
    "reformatted_rest_ner_results[:5]         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1505ab7c-e1e2-4fca-a4cd-48dfc3a9d570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = []\n",
    "\n",
    "for tokens, ner_tags in zip(dict_rest_ner_test_samples['sentence'], reformatted_rest_ner_results):\n",
    "    \n",
    "    pred = []\n",
    "    for token in tokens:\n",
    "        if token not in ner_tags.keys():\n",
    "            pred.append('O')\n",
    "            y_pred.append('O')\n",
    "        else:\n",
    "            pred.append(ner_tags[token])\n",
    "            y_pred.append(ner_tags[token])\n",
    "    # print(tokens, ner_tags, pred)\n",
    "    \n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63fbc632-4a15-4f77-ab09-aabd77bf63d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10096"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b74c09b1-d83d-4450-bd7c-01882348780e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10096"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = []\n",
    "for each in dict_rest_ner_test_samples['label']:\n",
    "    y_true.extend(each)\n",
    "\n",
    "len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "198401eb-d48b-4939-bf03-af6118f376d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9514659270998416"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91365cc9-27ba-4fe6-839f-a26d698e1e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.76714801, 0.66949153, 0.96958951]),\n",
       " array([0.69444444, 0.56428571, 0.98033464]),\n",
       " array([0.72898799, 0.6124031 , 0.97493247]),\n",
       " array([ 612,  280, 9204]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_recall_fscore_support(y_true, y_pred, labels=['B', 'I', 'O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "838f679c-1220-4434-85d7-cd4ebe8a9a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 425,    9,  178],\n",
       "       [  17,  158,  105],\n",
       "       [ 112,   69, 9023]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_true, y_pred, labels=['B', 'I', 'O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "68129bab-3994-4d9e-8753-c4a0d2eda5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bab5371e-97fd-4884-82ef-89f322ce6cd7",
   "metadata": {},
   "source": [
    "## Home Work: do laptop NER (AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a0dc7-dd22-47d9-821a-f5c14c69086e",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea35995a-5b29-423d-9d1f-94bc496fa9c6",
   "metadata": {},
   "source": [
    "## Load Testing  - to do..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d81dd-30c8-480d-85e7-773da820929b",
   "metadata": {},
   "source": [
    "### Config File"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5e615670-d534-436c-a182-d938411c19c5",
   "metadata": {},
   "source": [
    "### master.conf\n",
    "locustfile = locust_ae.py\n",
    "headless = true\n",
    "master = false\n",
    "expect-workers = 0\n",
    "host = http://127.0.0.1:1188/invocations\n",
    "users = 10\n",
    "spawn-rate = 1\n",
    "run-time = 5m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73a88de-474d-4946-8faa-d00379044c7e",
   "metadata": {},
   "source": [
    "### Result - Distil Bert"
   ]
  },
  {
   "cell_type": "raw",
   "id": "233aa2bb-6409-4e08-a2b7-11f9b6cc07eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
    "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
    "POST     /invocations                                                                     943     0(0.00%) |     70      41     162     69 |    3.15        0.00\n",
    "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
    "         Aggregated                                                                       943     0(0.00%) |     70      41     162     69 |    3.15        0.00\n",
    "\n",
    "Response time percentiles (approximated)\n",
    "Type     Name                                                                                  50%    66%    75%    80%    90%    95%    98%    99%  99.9% 99.99%   100% # reqs\n",
    "--------|--------------------------------------------------------------------------------|--------|------|------|------|------|------|------|------|------|------|------|------\n",
    "POST     /invocations                                                                           69     76     81     87     97    110    130    140    160    160    160    943\n",
    "--------|--------------------------------------------------------------------------------|--------|------|------|------|------|------|------|------|------|------|------|------\n",
    "         Aggregated                                                                             69     76     81     87     97    110    130    140    160    160    160    943"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055944db-2aff-4792-967f-ed727378970b",
   "metadata": {},
   "source": [
    "### Result - Bert"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07e55607-dba5-4883-891e-6d323699af31",
   "metadata": {},
   "source": [
    "Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
    "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
    "POST     /invocations                                                                     947     0(0.00%) |    130      76     360    120 |    3.16        0.00\n",
    "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
    "         Aggregated                                                                       947     0(0.00%) |    130      76     360    120 |    3.16        0.00\n",
    "\n",
    "Response time percentiles (approximated)\n",
    "Type     Name                                                                                  50%    66%    75%    80%    90%    95%    98%    99%  99.9% 99.99%   100% # reqs\n",
    "--------|--------------------------------------------------------------------------------|--------|------|------|------|------|------|------|------|------|------|------|------\n",
    "POST     /invocations                                                                          120    140    150    160    190    220    250    290    360    360    360    947\n",
    "--------|--------------------------------------------------------------------------------|--------|------|------|------|------|------|------|------|------|------|------|------\n",
    "         Aggregated                                                                            120    140    150    160    190    220    250    290    360    360    360    947"
   ]
  },
  {
   "cell_type": "raw",
   "id": "010ddd33-2b75-47f8-912f-5ed9b59403f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a04d7fa-74fd-427d-a2fe-b8bf3ba8cee7",
   "metadata": {},
   "source": [
    "### Config File"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b257c82e-b978-4ff2-936d-015a2f4646b0",
   "metadata": {},
   "source": [
    "### master.conf\n",
    "locustfile = locust_ae.py\n",
    "headless = true\n",
    "master = false\n",
    "expect-workers = 0\n",
    "host = http://127.0.0.1:1188/invocations\n",
    "users = 20\n",
    "spawn-rate = 2\n",
    "run-time = 5m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f229b5-0323-4eb9-afb8-410592f33c0c",
   "metadata": {},
   "source": [
    "### Result - Distil Bert"
   ]
  },
  {
   "cell_type": "raw",
   "id": "411639c0-e06e-4570-9881-f8448e9c1218",
   "metadata": {},
   "source": [
    "Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
    "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
    "POST     /invocations                                                                    1967     0(0.00%) |     72      41     208     66 |    6.57        0.00\n",
    "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
    "         Aggregated                                                                      1967     0(0.00%) |     72      41     208     66 |    6.57        0.00\n",
    "\n",
    "Response time percentiles (approximated)\n",
    "Type     Name                                                                                  50%    66%    75%    80%    90%    95%    98%    99%  99.9% 99.99%   100% # reqs\n",
    "--------|--------------------------------------------------------------------------------|--------|------|------|------|------|------|------|------|------|------|------|------\n",
    "POST     /invocations                                                                           66     75     84     92    110    130    150    150    200    210    210   1967\n",
    "--------|--------------------------------------------------------------------------------|--------|------|------|------|------|------|------|------|------|------|------|------\n",
    "         Aggregated                                                                             66     75     84     92    110    130    150    150    200    210    210   1967"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abde2667-11b5-4e79-ae70-1c3559a6309e",
   "metadata": {},
   "source": [
    "### Result - Bert"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d7eaf584-5782-4cee-bd53-1a6844f55870",
   "metadata": {},
   "source": [
    "Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
    "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
    "POST     /invocations                                                                    1854     0(0.00%) |    165      76     503    140 |    6.18        0.00\n",
    "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
    "         Aggregated                                                                      1854     0(0.00%) |    165      76     503    140 |    6.18        0.00\n",
    "\n",
    "Response time percentiles (approximated)\n",
    "Type     Name                                                                                  50%    66%    75%    80%    90%    95%    98%    99%  99.9% 99.99%   100% # reqs\n",
    "--------|--------------------------------------------------------------------------------|--------|------|------|------|------|------|------|------|------|------|------|------\n",
    "POST     /invocations                                                                          140    180    210    230    280    320    370    390    480    500    500   1854\n",
    "--------|--------------------------------------------------------------------------------|--------|------|------|------|------|------|------|------|------|------|------|------\n",
    "         Aggregated                                                                            140    180    210    230    280    320    370    390    480    500    500   1854"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9b63cbf-30e1-4387-89fb-257014573d41",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
