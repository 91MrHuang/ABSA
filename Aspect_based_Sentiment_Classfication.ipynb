{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb30ef96-e70f-402a-b62f-e8987b7498bf",
   "metadata": {},
   "source": [
    "## Sentitment by Finetuning Bert model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa09e21-a4f3-4618-b11b-3145416a0929",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2553f4a3-5c57-4448-ba9c-32fbf7b28a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1996450-82a9-43e5-a93e-2ea885f6bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "import requests\n",
    "request = requests.get(\"https://drive.google.com/uc?export=download&id=1wHt8PsMLsfX5yNSqrt2fSTcb8LEiclcf\")\n",
    "with open(\"data.zip\", \"wb\") as file:\n",
    "    file.write(request.content)\n",
    "\n",
    "# Unzip data\n",
    "import zipfile\n",
    "with zipfile.ZipFile('data.zip') as zip:\n",
    "    zip.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9618641c-61d7-44ab-a1e5-8917f2e33e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"@united I'm having issues. Yesterday I rebooked for 24 hours after I was supposed to fly, now I can't log on &amp; check in. Can you help?\", \"@united kinda feel like the $6.99 you charge for in flight Wi-Fi is ridiculous. AND it sucks, slow, or doesn't work. #anythingtomakeabuck\", 'Livid in Vegas, delayed, again&amp; again&amp;again, @SouthwestAir decided to cancel a flight and combine two, then waited on crew, now pilots.', '@united the most annoying man on earth is on my flight. what can you do to help me?', \"@united The last 2 weeks I've flown wit u, you have given me 4 reasons to convince me it was a bad decision. Time 2 go back 2 @SouthwestAir\"]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>134256</td>\n",
       "      <td>@AmericanAir I just found your breach! I don't...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>14116</td>\n",
       "      <td>@flyLAXairport @AmericanAir @FlyEIA Odd that t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2538</th>\n",
       "      <td>78337</td>\n",
       "      <td>Hey @SouthwestAir will you have more nonstop f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>157276</td>\n",
       "      <td>Dad: Delta Charged Me $88 Fee to Sit Next to 4...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>38242</td>\n",
       "      <td>Only @JetBlue would value investors over custo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  label\n",
       "537   134256  @AmericanAir I just found your breach! I don't...      0\n",
       "1822   14116  @flyLAXairport @AmericanAir @FlyEIA Odd that t...      1\n",
       "2538   78337  Hey @SouthwestAir will you have more nonstop f...      1\n",
       "395   157276  Dad: Delta Charged Me $88 Fee to Sit Next to 4...      0\n",
       "2156   38242  Only @JetBlue would value investors over custo...      1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Load data and set labels\n",
    "data_complaint = pd.read_csv('data/complaint1700.csv')\n",
    "data_complaint['label'] = 0\n",
    "data_non_complaint = pd.read_csv('data/noncomplaint1700.csv')\n",
    "data_non_complaint['label'] = 1\n",
    "\n",
    "# Concatenate complaining and non-complaining data\n",
    "data = pd.concat([data_complaint, data_non_complaint], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Drop 'airline' column\n",
    "data.drop(['airline'], inplace=True, axis=1)\n",
    "\n",
    "# Display 5 random samples\n",
    "print(list(data.head(5)['tweet']))\n",
    "print(list(data.head(5)['label']))\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07b8e339-c33b-4f97-b37b-16929da854fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       3400\n",
       "tweet    3400\n",
       "label    3400\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe83fb79-cb9d-47e5-91f8-57b73b945268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tweet_length = max([len(each) for each in list(data['tweet'])])\n",
    "max_tweet_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db4d9b26-77e8-477a-9b66-41ded4d68305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.tweet.values\n",
    "y = data.label.values\n",
    "\n",
    "X_train, X_val, y_train, y_val =\\\n",
    "    train_test_split(X, y, test_size=0.1, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d8ac39b-0127-482a-9ac8-92629fe1fb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>42892</td>\n",
       "      <td>@AmericanAir please tell me how it's possible ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3473</th>\n",
       "      <td>131396</td>\n",
       "      <td>Hiiiii @DeltaAssist I've been on hold for over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2715</th>\n",
       "      <td>103943</td>\n",
       "      <td>@BTSullivan91 @AmericanAir why are they stuck ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>31230</td>\n",
       "      <td>@AmericanAir Why the heck did you yank your Wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>42436</td>\n",
       "      <td>Sad but very true. @united  http://t.co/O282zo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet\n",
       "1091   42892  @AmericanAir please tell me how it's possible ...\n",
       "3473  131396  Hiiiii @DeltaAssist I've been on hold for over...\n",
       "2715  103943  @BTSullivan91 @AmericanAir why are they stuck ...\n",
       "780    31230  @AmericanAir Why the heck did you yank your Wi...\n",
       "1065   42436  Sad but very true. @united  http://t.co/O282zo..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test data\n",
    "test_data = pd.read_csv('data/test_data.csv')\n",
    "\n",
    "# Keep important columns\n",
    "test_data = test_data[['id', 'tweet']]\n",
    "\n",
    "# Display 5 samples from the test data\n",
    "test_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b1ff0967-76a2-478b-98b3-083dfe3cee4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tweet_length = max([len(each.split(' ')) for each in list(test_data['tweet'])])\n",
    "max_tweet_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674d931-f68a-40f8-9cc1-e1f07a51636a",
   "metadata": {},
   "source": [
    "### Bert model finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "85017371-30f9-4c60-9847-e3b0e9cedc51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /Users/sean.huang/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /Users/sean.huang/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /Users/sean.huang/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /Users/sean.huang/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /Users/sean.huang/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "pretrained_bert_model_name = 'bert-base-uncased'\n",
    "pretrained_bert_tokenizer = AutoTokenizer.from_pretrained(pretrained_bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b68149df-aa70-43ae-b771-027ea14179f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65da39ef3fb49b4a033615a4342700d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36a2ff6060c46ae89ef432f1c24eec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def prepare_classification_labels(samples):\n",
    "    inputs = pretrained_bert_tokenizer(samples['tweet'], truncation=True, padding='max_length', max_length=32, is_split_into_words=False)\n",
    "    \n",
    "    # print('inputs', inputs)\n",
    "    # print('inputs', inputs.keys())\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "train_datasets = Dataset.from_pandas(data[:300]).map(prepare_classification_labels, batched=True, batch_size=100).remove_columns(['id', 'tweet'])\n",
    "validation_datasets = Dataset.from_pandas(data[300:340]).map(prepare_classification_labels, batched=True, batch_size=10).remove_columns(['id', 'tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fb2d5f9b-6613-4938-b791-25ba89ea6d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /Users/sean.huang/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"positive\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /Users/sean.huang/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pretrained_bert_model = AutoModelForSequenceClassification.from_pretrained(pretrained_bert_model_name, num_labels=2, id2label={0:'negative', 1:'positive'}, label2id={'negative':0, 'positive':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "21682ed9-c12b-42ce-9a60-cb0884506e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch \n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')   # mps is for Apple M1 GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fdf3c6f8-0585-46e0-af63-5bf6620deca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "torch.distributed process group is initialized, but local_rank == -1. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "***** Running training *****\n",
      "  Num examples = 300\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:27, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.025234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.252200</td>\n",
       "      <td>0.003022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.001119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.000690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./sentiment_test_model\n",
      "Configuration saved in ./sentiment_test_model/config.json\n",
      "Model weights saved in ./sentiment_test_model/pytorch_model.bin\n",
      "tokenizer config file saved in ./sentiment_test_model/tokenizer_config.json\n",
      "Special tokens file saved in ./sentiment_test_model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./sentiment_test_model',\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    logging_strategy='steps',\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps = 10,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model=pretrained_bert_model,\n",
    "    args=args,\n",
    "    train_dataset=train_datasets,\n",
    "    eval_dataset=validation_datasets,\n",
    "    tokenizer=pretrained_bert_tokenizer,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc0bf8f-5372-419b-a381-1de99ea29e82",
   "metadata": {},
   "source": [
    "## Restaurant ASC(Aspect based Sentiment Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c69e380-a49d-425c-8028-a81f369bf410",
   "metadata": {},
   "source": [
    "### Preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c63d837-99fb-41a5-b887-8a0f27e9fdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3452"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "rest_asc_training_file = '/Users/sean.huang/Documents/my/books and courses/ztgg/Shared Code/BERT-for-ABSA/asc/rest/train.json'\n",
    "rest_asc_training_samples = json.load(open(rest_asc_training_file))\n",
    "len(rest_asc_training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92074c0d-abed-480c-8f6f-95b77f5e98e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'polarity': 'positive',\n",
       " 'term': 'server',\n",
       " 'id': '1592_0',\n",
       " 'sentence': 'Our server was very helpful and friendly.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_asc_training_samples['1592_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38a7078e-fb0a-47ef-b6df-21a5776f543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rest_asc_training_samples = {'sample_id':[], 'sentence':[], 'term':[], 'label':[]}\n",
    "\n",
    "for key in rest_asc_training_samples.keys():\n",
    "    dict_rest_asc_training_samples['sample_id'].append(key)\n",
    "    dict_rest_asc_training_samples['sentence'].append(rest_asc_training_samples[key]['sentence'])\n",
    "    dict_rest_asc_training_samples['term'].append(rest_asc_training_samples[key]['term'])\n",
    "    dict_rest_asc_training_samples['label'].append(0 if rest_asc_training_samples[key]['polarity'] == 'negative' else 1 if rest_asc_training_samples[key]['polarity'] == 'positive' else 2)\n",
    "    \n",
    "# dict_rest_asc_training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2547492-b5d9-443e-a31a-741a76ff0c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>term</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1592_0</td>\n",
       "      <td>Our server was very helpful and friendly.</td>\n",
       "      <td>server</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1940_1</td>\n",
       "      <td>Super friendly and knowledgable staff, fabulou...</td>\n",
       "      <td>staff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1940_0</td>\n",
       "      <td>Super friendly and knowledgable staff, fabulou...</td>\n",
       "      <td>jazz brunch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1940_3</td>\n",
       "      <td>Super friendly and knowledgable staff, fabulou...</td>\n",
       "      <td>live jazz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1940_2</td>\n",
       "      <td>Super friendly and knowledgable staff, fabulou...</td>\n",
       "      <td>bistro fare</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3447</th>\n",
       "      <td>3139_2</td>\n",
       "      <td>Where tanks in other Chinatown restaurants dis...</td>\n",
       "      <td>tanks</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3448</th>\n",
       "      <td>3139_1</td>\n",
       "      <td>Where tanks in other Chinatown restaurants dis...</td>\n",
       "      <td>dim sum</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>3139_0</td>\n",
       "      <td>Where tanks in other Chinatown restaurants dis...</td>\n",
       "      <td>tanks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3450</th>\n",
       "      <td>2818_0</td>\n",
       "      <td>This was my frist time at Cafe St. Bart's and ...</td>\n",
       "      <td>service</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3451</th>\n",
       "      <td>2818_1</td>\n",
       "      <td>This was my frist time at Cafe St. Bart's and ...</td>\n",
       "      <td>food</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3452 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample_id                                           sentence  \\\n",
       "0       1592_0          Our server was very helpful and friendly.   \n",
       "1       1940_1  Super friendly and knowledgable staff, fabulou...   \n",
       "2       1940_0  Super friendly and knowledgable staff, fabulou...   \n",
       "3       1940_3  Super friendly and knowledgable staff, fabulou...   \n",
       "4       1940_2  Super friendly and knowledgable staff, fabulou...   \n",
       "...        ...                                                ...   \n",
       "3447    3139_2  Where tanks in other Chinatown restaurants dis...   \n",
       "3448    3139_1  Where tanks in other Chinatown restaurants dis...   \n",
       "3449    3139_0  Where tanks in other Chinatown restaurants dis...   \n",
       "3450    2818_0  This was my frist time at Cafe St. Bart's and ...   \n",
       "3451    2818_1  This was my frist time at Cafe St. Bart's and ...   \n",
       "\n",
       "             term  label  \n",
       "0          server      1  \n",
       "1           staff      1  \n",
       "2     jazz brunch      1  \n",
       "3       live jazz      1  \n",
       "4     bistro fare      1  \n",
       "...           ...    ...  \n",
       "3447        tanks      0  \n",
       "3448      dim sum      2  \n",
       "3449        tanks      1  \n",
       "3450      service      1  \n",
       "3451         food      1  \n",
       "\n",
       "[3452 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rest_asc_training_samples = pd.DataFrame.from_dict(dict_rest_asc_training_samples)\n",
    "df_rest_asc_training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba814ef-3a40-4cf1-9b01-aa2cca0aaafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sent_length = max([len(each.split(' ')) for each in list(df_rest_asc_training_samples['sentence'])])\n",
    "max_sent_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f5eff36-9064-434f-afdc-b4584a49c32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 2094, 0: 779, 2: 579})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(df_rest_asc_training_samples['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a673aa52-d5b5-4d36-8b2c-0b9662c90265",
   "metadata": {},
   "source": [
    "### Load and finetune pretrained Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f86445d-2fe0-4679-8b8b-162035bbf4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "pretrained_bert_model_name = 'bert-base-uncased'\n",
    "pretrained_bert_tokenizer = AutoTokenizer.from_pretrained(pretrained_bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb89b15c-2494-482f-b203-e9388a0eb8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2256, 8241, 2001, 2200, 14044, 1998, 5379, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pretrained_bert_tokenizer(list(df_rest_asc_training_samples['sentence'])[0], is_split_into_words=False)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f357168-5e30-4634-aa53-fb032b392203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Our server was very helpful and friendly.', 'server')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = list(df_rest_asc_training_samples['sentence'])[0]\n",
    "term1 = list(df_rest_asc_training_samples['term'])[0]\n",
    "sentence1, term1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d54c80e2-6480-4a3b-b08e-389b7d640ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2256, 8241, 2001, 2200, 14044, 1998, 5379, 1012, 102, 8241, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pretrained_bert_tokenizer(sentence1, term1, is_split_into_words=False)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92df943d-4a06-444a-8550-13826e55155b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2256, 8241, 2001, 2200, 14044, 1998, 5379, 1012, 102, 8241, 102, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pretrained_bert_tokenizer(sentence1, term1, truncation=True, padding='max_length', max_length=16, is_split_into_words=False)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854b560-4e60-49e8-a4ed-87cd6f7a1277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80c2b479-16ba-403b-b0b9-32297fa3516e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function prepare_asc_classification_labels at 0x15b37d550> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6cab74e2d2407c8816dff4850af1ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f695d1c88d641c19a53751e8b4e5283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def prepare_asc_classification_labels(samples):\n",
    "    inputs = pretrained_bert_tokenizer(samples['sentence'], samples['term'], truncation=True, padding='max_length', max_length=128, is_split_into_words=False)\n",
    "    \n",
    "    # print('inputs', inputs)\n",
    "    # print('inputs', inputs.keys())\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "asc_train_datasets = Dataset.from_pandas(df_rest_asc_training_samples[:3000]).map(prepare_asc_classification_labels, batched=True, batch_size=1000).remove_columns(['sample_id', 'sentence', 'term'])\n",
    "asc_validation_datasets = Dataset.from_pandas(df_rest_asc_training_samples[3000:]).map(prepare_asc_classification_labels, batched=True, batch_size=100).remove_columns(['sample_id', 'sentence', 'term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04263464-c4ac-41e7-9ee4-7ac0945cdcf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asc_train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c65fd781-315c-4354-9e0e-26f4530b1f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 452\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asc_validation_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "425807bd-84b4-41bd-9292-47500351035b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')   # mps is for Apple M1 GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7887b86-ba86-4e98-825c-26ce76278b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import torch\n",
    "\n",
    "pretrained_bert_model = AutoModelForSequenceClassification.from_pretrained(pretrained_bert_model_name, num_labels=3, id2label={0:'negative', 1:'positive', 2:'neutral'}, label2id={'negative':0, 'positive':1, 'neutral':2})\n",
    "\n",
    "pretrained_bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d4f98c7-fd0d-4ef7-9b79-3099b4dc20e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric('accuracy')\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dc24d66-e6ed-4a02-8166-ba5390ed0c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torch.distributed process group is initialized, but local_rank == -1. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "/Users/sean.huang/miniconda3/envs/m1_new/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 480\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [480/480 1:55:41, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.848100</td>\n",
       "      <td>0.717173</td>\n",
       "      <td>0.701327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.569900</td>\n",
       "      <td>0.734159</td>\n",
       "      <td>0.712389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>0.646147</td>\n",
       "      <td>0.774336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.634331</td>\n",
       "      <td>0.780973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.176700</td>\n",
       "      <td>0.640574</td>\n",
       "      <td>0.800885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.109600</td>\n",
       "      <td>0.831753</td>\n",
       "      <td>0.785398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.087300</td>\n",
       "      <td>0.975085</td>\n",
       "      <td>0.783186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.058700</td>\n",
       "      <td>0.791748</td>\n",
       "      <td>0.798673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.038600</td>\n",
       "      <td>0.983296</td>\n",
       "      <td>0.809735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.027200</td>\n",
       "      <td>0.944392</td>\n",
       "      <td>0.809735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.026500</td>\n",
       "      <td>1.012469</td>\n",
       "      <td>0.798673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>1.052613</td>\n",
       "      <td>0.814159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>1.144214</td>\n",
       "      <td>0.809735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>1.138525</td>\n",
       "      <td>0.807522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.008700</td>\n",
       "      <td>1.147436</td>\n",
       "      <td>0.818584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>1.178824</td>\n",
       "      <td>0.816372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>1.231813</td>\n",
       "      <td>0.807522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.007900</td>\n",
       "      <td>1.212888</td>\n",
       "      <td>0.807522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>1.201737</td>\n",
       "      <td>0.809735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./rest_asc_model\n",
      "Configuration saved in ./rest_asc_model/config.json\n",
      "Model weights saved in ./rest_asc_model/pytorch_model.bin\n",
      "tokenizer config file saved in ./rest_asc_model/tokenizer_config.json\n",
      "Special tokens file saved in ./rest_asc_model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./rest_asc_model',\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=25,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps = 25,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_accuracy',\n",
    "    report_to='none'\n",
    ")\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model=pretrained_bert_model,\n",
    "    args=args,\n",
    "    train_dataset=asc_train_datasets,\n",
    "    eval_dataset=asc_validation_datasets,\n",
    "    tokenizer=pretrained_bert_tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171091e7-d07a-47a9-8110-4516f2b1e07a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "11aaa8fd-2a17-4aff-99a2-e7a9c09168e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d9ddf23a-c832-4b80-a0ec-b27c6addcb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c9f9d31-a824-4203-ac6d-153f96dd6778",
   "metadata": {},
   "source": [
    "### Load finetune Bert model for Rest NER task, and do prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "282e5c3f-6c8f-4864-8cf5-6ef1d9820681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./rest_asc_model/added_tokens.json. We won't load it.\n",
      "loading file ./rest_asc_model/vocab.txt\n",
      "loading file ./rest_asc_model/tokenizer.json\n",
      "loading file None\n",
      "loading file ./rest_asc_model/special_tokens_map.json\n",
      "loading file ./rest_asc_model/tokenizer_config.json\n",
      "loading configuration file ./rest_asc_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./rest_asc_model\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 2,\n",
      "    \"positive\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./rest_asc_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./rest_asc_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "finetuned_rest_asc_tokenizer = AutoTokenizer.from_pretrained('./rest_asc_model')\n",
    "finetuned_rest_asc_model = AutoModelForSequenceClassification.from_pretrained('./rest_asc_model', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "372bcb58-41a8-4f39-8a2c-28c4048ffd46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'negative', 1: 'positive', 2: 'neutral'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_labels = finetuned_rest_asc_model.config.id2label\n",
    "sentiment_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "194ce7c4-5829-4d92-9312-317852e91efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 4.3336, -0.8536, -3.9473]], grad_fn=<AddmmBackward0>)\n",
      "Sentiment:  negative\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tokens = finetuned_rest_asc_tokenizer('very terrible service, price is high, but taste is good.', is_split_into_words=False)\n",
    "\n",
    "output = finetuned_rest_asc_model(torch.tensor([tokens['input_ids']]))\n",
    "print('Logits:', output.logits)\n",
    "print('Sentiment: ', sentiment_labels[np.argmax(output.logits.tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45ba6590-ad2c-4a88-b5c1-9345f8090d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 5.5690, -2.5438, -3.1783]], grad_fn=<AddmmBackward0>)\n",
      "Sentiment:  negative\n"
     ]
    }
   ],
   "source": [
    "tokens = finetuned_rest_asc_tokenizer('very terrible service, price is high, but taste is good.', 'service', is_split_into_words=False)\n",
    "\n",
    "output = finetuned_rest_asc_model(torch.tensor([tokens['input_ids']]))\n",
    "print('Logits:', output.logits)\n",
    "print('Sentiment: ', sentiment_labels[np.argmax(output.logits.tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e318df76-323b-439d-80c0-866b4fd24b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 5.4031, -2.1838, -3.3275]], grad_fn=<AddmmBackward0>)\n",
      "Sentiment:  negative\n"
     ]
    }
   ],
   "source": [
    "tokens = finetuned_rest_asc_tokenizer('very terrible service, price is high, but taste is good.', 'price', is_split_into_words=False)\n",
    "output = finetuned_rest_asc_model(torch.tensor([tokens['input_ids']]))\n",
    "print('Logits:', output.logits)\n",
    "print('Sentiment: ', sentiment_labels[np.argmax(output.logits.tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "70718041-5f39-4a5c-877c-d55e5bfd5417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-3.0090,  5.7663, -3.1077]], grad_fn=<AddmmBackward0>)\n",
      "Sentiment:  positive\n"
     ]
    }
   ],
   "source": [
    "tokens = finetuned_rest_asc_tokenizer('very terrible service, price is high, but taste is good.', 'taste', is_split_into_words=False)\n",
    "output = finetuned_rest_asc_model(torch.tensor([tokens['input_ids']]))\n",
    "print('Logits:', output.logits)\n",
    "print('Sentiment: ', sentiment_labels[np.argmax(output.logits.tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6907aa7-ebc9-4196-a9a5-c4103eeedf6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dc90c51-5e8d-4551-8f36-9e47c52f41a7",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08906137-86d8-444e-b74f-de92b3bae543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1120"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_asc_test_file = '/Users/sean.huang/Documents/my/books and courses/ztgg/Shared Code/BERT-for-ABSA/asc/rest/test.json'\n",
    "rest_asc_test_samples = json.load(open(rest_asc_test_file))\n",
    "len(rest_asc_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2ffac951-19bd-4d44-bf56-06b5fa303cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def asc(sentence, term):\n",
    "\n",
    "    tokens = finetuned_rest_asc_tokenizer(sentence, term, is_split_into_words=False)\n",
    "    output = finetuned_rest_asc_model(torch.tensor([tokens['input_ids']]))\n",
    "    \n",
    "    return sentiment_labels[np.argmax(output.logits.tolist())]\n",
    "\n",
    "asc('very terrible service, price is high, but taste is good.', 'taste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5ace1aa-9fbe-4733-9f34-4428a8ef3219",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for key in rest_asc_test_samples.keys():\n",
    "    y_pred.append(asc(rest_asc_test_samples[key]['sentence'], rest_asc_test_samples[key]['term']))\n",
    "    # print(rest_asc_test_samples[key]['sentence'])\n",
    "    # print(rest_asc_test_samples[key]['term'])\n",
    "    # print(asc(rest_asc_test_samples[key]['sentence'], rest_asc_test_samples[key]['term']))\n",
    "    y_true.append(rest_asc_test_samples[key]['polarity'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2c80be72-ef9d-4195-a3ac-23eba513cffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative'] ['positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative']\n",
      "['positive', 'positive', 'neutral', 'neutral', 'positive', 'negative', 'neutral', 'negative', 'negative', 'neutral'] ['positive', 'positive', 'neutral', 'neutral', 'positive', 'negative', 'neutral', 'negative', 'negative', 'neutral']\n"
     ]
    }
   ],
   "source": [
    "print(y_true[:10], y_true[:10])\n",
    "print(y_pred[:10], y_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b134d55e-e7ca-488a-9a4d-87bcbfd7d6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'positive': 728, 'negative': 196, 'neutral': 196})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f9c0f569-45c2-4093-bf7f-65e33d372c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.79787234, 0.89182058, 0.59770115]),\n",
       " array([0.76530612, 0.92857143, 0.53061224]),\n",
       " array([0.78125   , 0.90982503, 0.56216216]),\n",
       " array([196, 728, 196]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_true, y_pred, labels=['negative', 'positive', 'neutral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "67ed1212-fa6c-43b1-a801-a9ae3aacd56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[150,  15,  31],\n",
       "       [ 13, 676,  39],\n",
       "       [ 25,  67, 104]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_true, y_pred, labels=['negative', 'positive', 'neutral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b3071962-5a0a-48db-9f50-1810f42511ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8303571428571429"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe653ade-e04f-48b6-914f-5d251976dc65",
   "metadata": {},
   "source": [
    "## Home Work: do laptop ASC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3f4463-d165-4bf7-959f-97312940626b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
