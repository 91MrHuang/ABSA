{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb30ef96-e70f-402a-b62f-e8987b7498bf",
   "metadata": {},
   "source": [
    "## Sentitment by Finetuning Bert model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa09e21-a4f3-4618-b11b-3145416a0929",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2553f4a3-5c57-4448-ba9c-32fbf7b28a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1996450-82a9-43e5-a93e-2ea885f6bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "import requests\n",
    "request = requests.get(\"https://drive.google.com/uc?export=download&id=1wHt8PsMLsfX5yNSqrt2fSTcb8LEiclcf\")\n",
    "with open(\"data.zip\", \"wb\") as file:\n",
    "    file.write(request.content)\n",
    "\n",
    "# Unzip data\n",
    "import zipfile\n",
    "with zipfile.ZipFile('data.zip') as zip:\n",
    "    zip.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9618641c-61d7-44ab-a1e5-8917f2e33e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"@united I'm having issues. Yesterday I rebooked for 24 hours after I was supposed to fly, now I can't log on &amp; check in. Can you help?\", \"@united kinda feel like the $6.99 you charge for in flight Wi-Fi is ridiculous. AND it sucks, slow, or doesn't work. #anythingtomakeabuck\", 'Livid in Vegas, delayed, again&amp; again&amp;again, @SouthwestAir decided to cancel a flight and combine two, then waited on crew, now pilots.', '@united the most annoying man on earth is on my flight. what can you do to help me?', \"@united The last 2 weeks I've flown wit u, you have given me 4 reasons to convince me it was a bad decision. Time 2 go back 2 @SouthwestAir\"]\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>38466</td>\n",
       "      <td>Oh @JetBlue why hast thou forsaken us?? It was...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2386</th>\n",
       "      <td>62611</td>\n",
       "      <td>@MaksimC @united what's the problem. I'm w Pro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>154120</td>\n",
       "      <td>Ã¢â‚¬Å“@JustinNFJK: Guys I would totally fight...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>24256</td>\n",
       "      <td>@JLJeffLewis @AmericanAir @USAirways @Jennipul...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>73089</td>\n",
       "      <td>@AmericanAir Your Baggage Customer Service des...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  label\n",
       "627    38466  Oh @JetBlue why hast thou forsaken us?? It was...      0\n",
       "2386   62611  @MaksimC @united what's the problem. I'm w Pro...      1\n",
       "178   154120  Ã¢â‚¬Å“@JustinNFJK: Guys I would totally fight...      0\n",
       "1976   24256  @JLJeffLewis @AmericanAir @USAirways @Jennipul...      1\n",
       "521    73089  @AmericanAir Your Baggage Customer Service des...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Load data and set labels\n",
    "data_complaint = pd.read_csv('data/complaint1700.csv')\n",
    "data_complaint['label'] = 0\n",
    "data_non_complaint = pd.read_csv('data/noncomplaint1700.csv')\n",
    "data_non_complaint['label'] = 1\n",
    "\n",
    "# Concatenate complaining and non-complaining data\n",
    "data = pd.concat([data_complaint, data_non_complaint], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Drop 'airline' column\n",
    "data.drop(['airline'], inplace=True, axis=1)\n",
    "\n",
    "# Display 5 random samples\n",
    "print(list(data.head(5)['tweet']))\n",
    "print(list(data.head(5)['label']))\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07b8e339-c33b-4f97-b37b-16929da854fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       3400\n",
       "tweet    3400\n",
       "label    3400\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe83fb79-cb9d-47e5-91f8-57b73b945268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tweet_length = max([len(each) for each in list(data['tweet'])])\n",
    "max_tweet_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db4d9b26-77e8-477a-9b66-41ded4d68305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.tweet.values\n",
    "y = data.label.values\n",
    "\n",
    "X_train, X_val, y_train, y_val =\\\n",
    "    train_test_split(X, y, test_size=0.1, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d8ac39b-0127-482a-9ac8-92629fe1fb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>35663</td>\n",
       "      <td>@AmericanAir let's continue this discussion......</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>49247</td>\n",
       "      <td>@united I get that sometimes there's things yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>33067</td>\n",
       "      <td>@AmericanAir  what does priority mean? Clearly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2530</th>\n",
       "      <td>96817</td>\n",
       "      <td>Just had a very bad experience with the disser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>36275</td>\n",
       "      <td>18 1/2 hours later. Finally heading home. Wors...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              tweet\n",
       "899   35663  @AmericanAir let's continue this discussion......\n",
       "1252  49247  @united I get that sometimes there's things yo...\n",
       "828   33067  @AmericanAir  what does priority mean? Clearly...\n",
       "2530  96817  Just had a very bad experience with the disser...\n",
       "911   36275  18 1/2 hours later. Finally heading home. Wors..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test data\n",
    "test_data = pd.read_csv('data/test_data.csv')\n",
    "\n",
    "# Keep important columns\n",
    "test_data = test_data[['id', 'tweet']]\n",
    "\n",
    "# Display 5 samples from the test data\n",
    "test_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1ff0967-76a2-478b-98b3-083dfe3cee4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tweet_length = max([len(each.split(' ')) for each in list(test_data['tweet'])])\n",
    "max_tweet_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2674d931-f68a-40f8-9cc1-e1f07a51636a",
   "metadata": {},
   "source": [
    "### Bert model finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85017371-30f9-4c60-9847-e3b0e9cedc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "pretrained_bert_model_name = 'distilbert-base-uncased'\n",
    "pretrained_bert_tokenizer = AutoTokenizer.from_pretrained(pretrained_bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b68149df-aa70-43ae-b771-027ea14179f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function prepare_classification_labels at 0x298051c10> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200eb886aebf4477845404b6e1249a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27b6421523d4b32a7be268894b07cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def prepare_classification_labels(samples):\n",
    "    inputs = pretrained_bert_tokenizer(samples['tweet'], truncation=True, padding='max_length', max_length=32, is_split_into_words=False)\n",
    "    \n",
    "    # print('inputs', inputs)\n",
    "    # print('inputs', inputs.keys())\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "train_datasets = Dataset.from_pandas(data[:300]).map(prepare_classification_labels, batched=True, batch_size=100).remove_columns(['id', 'tweet'])\n",
    "validation_datasets = Dataset.from_pandas(data[300:340]).map(prepare_classification_labels, batched=True, batch_size=10).remove_columns(['id', 'tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d5f9b-6613-4938-b791-25ba89ea6d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_bert_model = AutoModelForSequenceClassification.from_pretrained(pretrained_bert_model_name, num_labels=2, id2label={0:'negative', 1:'positive'}, label2id={'negative':0, 'positive':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "21682ed9-c12b-42ce-9a60-cb0884506e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import torch \n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')   # mps is for Apple M1 GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fdf3c6f8-0585-46e0-af63-5bf6620deca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "torch.distributed process group is initialized, but local_rank == -1. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "***** Running training *****\n",
      "  Num examples = 300\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:27, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.025234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.252200</td>\n",
       "      <td>0.003022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.001119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.000690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 40\n",
      "  Batch size = 4\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./sentiment_test_model\n",
      "Configuration saved in ./sentiment_test_model/config.json\n",
      "Model weights saved in ./sentiment_test_model/pytorch_model.bin\n",
      "tokenizer config file saved in ./sentiment_test_model/tokenizer_config.json\n",
      "Special tokens file saved in ./sentiment_test_model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./sentiment_test_model',\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    logging_strategy='steps',\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps = 10,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='none'\n",
    ")\n",
    "\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model=pretrained_bert_model,\n",
    "    args=args,\n",
    "    train_dataset=train_datasets,\n",
    "    eval_dataset=validation_datasets,\n",
    "    tokenizer=pretrained_bert_tokenizer,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc0bf8f-5372-419b-a381-1de99ea29e82",
   "metadata": {},
   "source": [
    "## Restaurant ASC(Aspect based Sentiment Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c69e380-a49d-425c-8028-a81f369bf410",
   "metadata": {},
   "source": [
    "### Preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c63d837-99fb-41a5-b887-8a0f27e9fdf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3452"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "rest_asc_training_file = '/Users/sean.huang/Documents/my/books and courses/ztgg/Shared Code/BERT-for-ABSA/asc/rest/train.json'\n",
    "rest_asc_training_samples = json.load(open(rest_asc_training_file))\n",
    "len(rest_asc_training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92074c0d-abed-480c-8f6f-95b77f5e98e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'polarity': 'positive',\n",
       " 'term': 'server',\n",
       " 'id': '1592_0',\n",
       " 'sentence': 'Our server was very helpful and friendly.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_asc_training_samples['1592_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38a7078e-fb0a-47ef-b6df-21a5776f543c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rest_asc_training_samples = {'sample_id':[], 'sentence':[], 'term':[], 'label':[]}\n",
    "\n",
    "for key in rest_asc_training_samples.keys():\n",
    "    dict_rest_asc_training_samples['sample_id'].append(key)\n",
    "    dict_rest_asc_training_samples['sentence'].append(rest_asc_training_samples[key]['sentence'])\n",
    "    dict_rest_asc_training_samples['term'].append(rest_asc_training_samples[key]['term'])\n",
    "    dict_rest_asc_training_samples['label'].append(0 if rest_asc_training_samples[key]['polarity'] == 'negative' else 1 if rest_asc_training_samples[key]['polarity'] == 'positive' else 2)\n",
    "    \n",
    "# dict_rest_asc_training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2547492-b5d9-443e-a31a-741a76ff0c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>term</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1592_0</td>\n",
       "      <td>Our server was very helpful and friendly.</td>\n",
       "      <td>server</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1940_1</td>\n",
       "      <td>Super friendly and knowledgable staff, fabulou...</td>\n",
       "      <td>staff</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1940_0</td>\n",
       "      <td>Super friendly and knowledgable staff, fabulou...</td>\n",
       "      <td>jazz brunch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1940_3</td>\n",
       "      <td>Super friendly and knowledgable staff, fabulou...</td>\n",
       "      <td>live jazz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1940_2</td>\n",
       "      <td>Super friendly and knowledgable staff, fabulou...</td>\n",
       "      <td>bistro fare</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3447</th>\n",
       "      <td>3139_2</td>\n",
       "      <td>Where tanks in other Chinatown restaurants dis...</td>\n",
       "      <td>tanks</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3448</th>\n",
       "      <td>3139_1</td>\n",
       "      <td>Where tanks in other Chinatown restaurants dis...</td>\n",
       "      <td>dim sum</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>3139_0</td>\n",
       "      <td>Where tanks in other Chinatown restaurants dis...</td>\n",
       "      <td>tanks</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3450</th>\n",
       "      <td>2818_0</td>\n",
       "      <td>This was my frist time at Cafe St. Bart's and ...</td>\n",
       "      <td>service</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3451</th>\n",
       "      <td>2818_1</td>\n",
       "      <td>This was my frist time at Cafe St. Bart's and ...</td>\n",
       "      <td>food</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3452 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample_id                                           sentence  \\\n",
       "0       1592_0          Our server was very helpful and friendly.   \n",
       "1       1940_1  Super friendly and knowledgable staff, fabulou...   \n",
       "2       1940_0  Super friendly and knowledgable staff, fabulou...   \n",
       "3       1940_3  Super friendly and knowledgable staff, fabulou...   \n",
       "4       1940_2  Super friendly and knowledgable staff, fabulou...   \n",
       "...        ...                                                ...   \n",
       "3447    3139_2  Where tanks in other Chinatown restaurants dis...   \n",
       "3448    3139_1  Where tanks in other Chinatown restaurants dis...   \n",
       "3449    3139_0  Where tanks in other Chinatown restaurants dis...   \n",
       "3450    2818_0  This was my frist time at Cafe St. Bart's and ...   \n",
       "3451    2818_1  This was my frist time at Cafe St. Bart's and ...   \n",
       "\n",
       "             term  label  \n",
       "0          server      1  \n",
       "1           staff      1  \n",
       "2     jazz brunch      1  \n",
       "3       live jazz      1  \n",
       "4     bistro fare      1  \n",
       "...           ...    ...  \n",
       "3447        tanks      0  \n",
       "3448      dim sum      2  \n",
       "3449        tanks      1  \n",
       "3450      service      1  \n",
       "3451         food      1  \n",
       "\n",
       "[3452 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rest_asc_training_samples = pd.DataFrame.from_dict(dict_rest_asc_training_samples)\n",
    "df_rest_asc_training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ba814ef-3a40-4cf1-9b01-aa2cca0aaafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sent_length = max([len(each.split(' ')) for each in list(df_rest_asc_training_samples['sentence'])])\n",
    "max_sent_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f5eff36-9064-434f-afdc-b4584a49c32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 2094, 0: 779, 2: 579})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(df_rest_asc_training_samples['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a673aa52-d5b5-4d36-8b2c-0b9662c90265",
   "metadata": {},
   "source": [
    "### Load and finetune pretrained Bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f86445d-2fe0-4679-8b8b-162035bbf4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "pretrained_bert_model_name = 'distilbert-base-uncased'\n",
    "pretrained_bert_tokenizer = AutoTokenizer.from_pretrained(pretrained_bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb89b15c-2494-482f-b203-e9388a0eb8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2256, 8241, 2001, 2200, 14044, 1998, 5379, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pretrained_bert_tokenizer(list(df_rest_asc_training_samples['sentence'])[0], is_split_into_words=False)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f357168-5e30-4634-aa53-fb032b392203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Our server was very helpful and friendly.', 'server')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1 = list(df_rest_asc_training_samples['sentence'])[0]\n",
    "term1 = list(df_rest_asc_training_samples['term'])[0]\n",
    "sentence1, term1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d54c80e2-6480-4a3b-b08e-389b7d640ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2256, 8241, 2001, 2200, 14044, 1998, 5379, 1012, 102, 8241, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pretrained_bert_tokenizer(sentence1, term1, is_split_into_words=False)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92df943d-4a06-444a-8550-13826e55155b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2256, 8241, 2001, 2200, 14044, 1998, 5379, 1012, 102, 8241, 102, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = pretrained_bert_tokenizer(sentence1, term1, truncation=True, padding='max_length', max_length=16, is_split_into_words=False)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854b560-4e60-49e8-a4ed-87cd6f7a1277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80c2b479-16ba-403b-b0b9-32297fa3516e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7c9892665549f9a19a72781ca85747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18ecddf08494a5c9e15b8188e0cd4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def prepare_asc_classification_labels(samples):\n",
    "    inputs = pretrained_bert_tokenizer(samples['sentence'], samples['term'], truncation=True, padding='max_length', max_length=128, is_split_into_words=False)\n",
    "    \n",
    "    # print('inputs', inputs)\n",
    "    # print('inputs', inputs.keys())\n",
    "    \n",
    "    return inputs\n",
    "\n",
    "asc_train_datasets = Dataset.from_pandas(df_rest_asc_training_samples[:3000]).map(prepare_asc_classification_labels, batched=True, batch_size=1000).remove_columns(['sample_id', 'sentence', 'term'])\n",
    "asc_validation_datasets = Dataset.from_pandas(df_rest_asc_training_samples[3000:]).map(prepare_asc_classification_labels, batched=True, batch_size=100).remove_columns(['sample_id', 'sentence', 'term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04263464-c4ac-41e7-9ee4-7ac0945cdcf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asc_train_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c65fd781-315c-4354-9e0e-26f4530b1f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 452\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asc_validation_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "425807bd-84b4-41bd-9292-47500351035b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')   # mps is for Apple M1 GPU\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7887b86-ba86-4e98-825c-26ce76278b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import torch\n",
    "\n",
    "pretrained_bert_model = AutoModelForSequenceClassification.from_pretrained(pretrained_bert_model_name, num_labels=3, id2label={0:'negative', 1:'positive', 2:'neutral'}, label2id={'negative':0, 'positive':1, 'neutral':2})\n",
    "\n",
    "pretrained_bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d4f98c7-fd0d-4ef7-9b79-3099b4dc20e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric('accuracy')\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5dc24d66-e6ed-4a02-8166-ba5390ed0c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torch.distributed process group is initialized, but local_rank == -1. In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\n",
      "/Users/sean.huang/miniconda3/envs/m1_new/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 480\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [480/480 49:55, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.834500</td>\n",
       "      <td>0.707224</td>\n",
       "      <td>0.716814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.586900</td>\n",
       "      <td>0.670777</td>\n",
       "      <td>0.716814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.429600</td>\n",
       "      <td>0.591807</td>\n",
       "      <td>0.765487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.332100</td>\n",
       "      <td>0.582340</td>\n",
       "      <td>0.774336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.258900</td>\n",
       "      <td>0.603500</td>\n",
       "      <td>0.756637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.177200</td>\n",
       "      <td>0.723168</td>\n",
       "      <td>0.778761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.142600</td>\n",
       "      <td>0.692343</td>\n",
       "      <td>0.796460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.095600</td>\n",
       "      <td>0.739440</td>\n",
       "      <td>0.787611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.075600</td>\n",
       "      <td>0.867270</td>\n",
       "      <td>0.792035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.060100</td>\n",
       "      <td>0.901158</td>\n",
       "      <td>0.776549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.860789</td>\n",
       "      <td>0.780973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.044900</td>\n",
       "      <td>0.916431</td>\n",
       "      <td>0.783186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.027600</td>\n",
       "      <td>0.895994</td>\n",
       "      <td>0.774336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.024700</td>\n",
       "      <td>0.965758</td>\n",
       "      <td>0.783186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.021100</td>\n",
       "      <td>1.028193</td>\n",
       "      <td>0.794248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>1.033776</td>\n",
       "      <td>0.792035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>1.024819</td>\n",
       "      <td>0.792035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>1.035988</td>\n",
       "      <td>0.787611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>1.041858</td>\n",
       "      <td>0.794248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 452\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./rest_asc_model_distilbert\n",
      "Configuration saved in ./rest_asc_model_distilbert/config.json\n",
      "Model weights saved in ./rest_asc_model_distilbert/pytorch_model.bin\n",
      "tokenizer config file saved in ./rest_asc_model_distilbert/tokenizer_config.json\n",
      "Special tokens file saved in ./rest_asc_model_distilbert/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir='./rest_asc_model_distilbert',\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=25,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps = 25,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_accuracy',\n",
    "    report_to='none'\n",
    ")\n",
    "    \n",
    "trainer = Trainer(\n",
    "    model=pretrained_bert_model,\n",
    "    args=args,\n",
    "    train_dataset=asc_train_datasets,\n",
    "    eval_dataset=asc_validation_datasets,\n",
    "    tokenizer=pretrained_bert_tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171091e7-d07a-47a9-8110-4516f2b1e07a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "11aaa8fd-2a17-4aff-99a2-e7a9c09168e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d9ddf23a-c832-4b80-a0ec-b27c6addcb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c9f9d31-a824-4203-ac6d-153f96dd6778",
   "metadata": {},
   "source": [
    "### Load finetune Bert model for Rest NER task, and do prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "282e5c3f-6c8f-4864-8cf5-6ef1d9820681",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ./rest_asc_model_distilbert/added_tokens.json. We won't load it.\n",
      "loading file ./rest_asc_model_distilbert/vocab.txt\n",
      "loading file ./rest_asc_model_distilbert/tokenizer.json\n",
      "loading file None\n",
      "loading file ./rest_asc_model_distilbert/special_tokens_map.json\n",
      "loading file ./rest_asc_model_distilbert/tokenizer_config.json\n",
      "loading configuration file ./rest_asc_model_distilbert/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"./rest_asc_model_distilbert\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\",\n",
      "    \"2\": \"neutral\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"negative\": 0,\n",
      "    \"neutral\": 2,\n",
      "    \"positive\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file ./rest_asc_model_distilbert/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at ./rest_asc_model_distilbert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "finetuned_rest_asc_tokenizer = AutoTokenizer.from_pretrained('./rest_asc_model_distilbert')\n",
    "finetuned_rest_asc_model = AutoModelForSequenceClassification.from_pretrained('./rest_asc_model_distilbert', num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "372bcb58-41a8-4f39-8a2c-28c4048ffd46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'negative', 1: 'positive', 2: 'neutral'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_labels = finetuned_rest_asc_model.config.id2label\n",
    "sentiment_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "194ce7c4-5829-4d92-9312-317852e91efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 2.0011,  1.2939, -2.8973]], grad_fn=<AddmmBackward0>)\n",
      "Sentiment:  negative\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tokens = finetuned_rest_asc_tokenizer('very terrible service, price is high, but taste is good.', is_split_into_words=False)\n",
    "\n",
    "output = finetuned_rest_asc_model(torch.tensor([tokens['input_ids']]))\n",
    "print('Logits:', output.logits)\n",
    "print('Sentiment: ', sentiment_labels[np.argmax(output.logits.tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45ba6590-ad2c-4a88-b5c1-9345f8090d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 4.7387, -1.7003, -2.8472]], grad_fn=<AddmmBackward0>)\n",
      "Sentiment:  negative\n"
     ]
    }
   ],
   "source": [
    "tokens = finetuned_rest_asc_tokenizer('very terrible service, price is high, but taste is good.', 'service', is_split_into_words=False)\n",
    "\n",
    "output = finetuned_rest_asc_model(torch.tensor([tokens['input_ids']]))\n",
    "print('Logits:', output.logits)\n",
    "print('Sentiment: ', sentiment_labels[np.argmax(output.logits.tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e318df76-323b-439d-80c0-866b4fd24b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 4.7166, -1.5648, -2.8783]], grad_fn=<AddmmBackward0>)\n",
      "Sentiment:  negative\n"
     ]
    }
   ],
   "source": [
    "tokens = finetuned_rest_asc_tokenizer('very terrible service, price is high, but taste is good.', 'price', is_split_into_words=False)\n",
    "output = finetuned_rest_asc_model(torch.tensor([tokens['input_ids']]))\n",
    "print('Logits:', output.logits)\n",
    "print('Sentiment: ', sentiment_labels[np.argmax(output.logits.tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70718041-5f39-4a5c-877c-d55e5bfd5417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-2.2771,  5.2422, -2.5211]], grad_fn=<AddmmBackward0>)\n",
      "Sentiment:  positive\n"
     ]
    }
   ],
   "source": [
    "tokens = finetuned_rest_asc_tokenizer('very terrible service, price is high, but taste is good.', 'taste', is_split_into_words=False)\n",
    "output = finetuned_rest_asc_model(torch.tensor([tokens['input_ids']]))\n",
    "print('Logits:', output.logits)\n",
    "print('Sentiment: ', sentiment_labels[np.argmax(output.logits.tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a6907aa7-ebc9-4196-a9a5-c4103eeedf6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dc90c51-5e8d-4551-8f36-9e47c52f41a7",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08906137-86d8-444e-b74f-de92b3bae543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1120"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_asc_test_file = '/Users/sean.huang/Documents/my/books and courses/ztgg/Shared Code/BERT-for-ABSA/asc/rest/test.json'\n",
    "rest_asc_test_samples = json.load(open(rest_asc_test_file))\n",
    "len(rest_asc_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ffac951-19bd-4d44-bf56-06b5fa303cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def asc(sentence, term):\n",
    "\n",
    "    tokens = finetuned_rest_asc_tokenizer(sentence, term, is_split_into_words=False)\n",
    "    output = finetuned_rest_asc_model(torch.tensor([tokens['input_ids']]))\n",
    "    \n",
    "    return sentiment_labels[np.argmax(output.logits.tolist())]\n",
    "\n",
    "asc('very terrible service, price is high, but taste is good.', 'taste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5ace1aa-9fbe-4733-9f34-4428a8ef3219",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for key in rest_asc_test_samples.keys():\n",
    "    y_pred.append(asc(rest_asc_test_samples[key]['sentence'], rest_asc_test_samples[key]['term']))\n",
    "    # print(rest_asc_test_samples[key]['sentence'])\n",
    "    # print(rest_asc_test_samples[key]['term'])\n",
    "    # print(asc(rest_asc_test_samples[key]['sentence'], rest_asc_test_samples[key]['term']))\n",
    "    y_true.append(rest_asc_test_samples[key]['polarity'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c80be72-ef9d-4195-a3ac-23eba513cffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative'] ['positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative']\n",
      "['positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative'] ['positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative']\n"
     ]
    }
   ],
   "source": [
    "print(y_true[:10], y_true[:10])\n",
    "print(y_pred[:10], y_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b134d55e-e7ca-488a-9a4d-87bcbfd7d6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'positive': 728, 'negative': 196, 'neutral': 196})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9c0f569-45c2-4093-bf7f-65e33d372c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.78217822, 0.87279597, 0.7016129 ]),\n",
       " array([0.80612245, 0.95192308, 0.44387755]),\n",
       " array([0.79396985, 0.91064389, 0.54375   ]),\n",
       " array([196, 728, 196]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_true, y_pred, labels=['negative', 'positive', 'neutral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "67ed1212-fa6c-43b1-a801-a9ae3aacd56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[158,  19,  19],\n",
       "       [ 17, 693,  18],\n",
       "       [ 27,  82,  87]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_true, y_pred, labels=['negative', 'positive', 'neutral'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b3071962-5a0a-48db-9f50-1810f42511ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8375"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe653ade-e04f-48b6-914f-5d251976dc65",
   "metadata": {},
   "source": [
    "## Home Work: do laptop ASC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aace2c-43c3-4c1c-93d5-5168cc917a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a1de316-7461-4776-bb2f-4438eba2b485",
   "metadata": {},
   "source": [
    "## Load Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da872b6-2836-48d3-9eee-36a690296708",
   "metadata": {},
   "source": [
    "### Config File"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e039f92c-e41c-4d0a-aebb-ca5939303a12",
   "metadata": {},
   "source": [
    "# master.conf in current directory\n",
    "locustfile = locust_asc.py\n",
    "headless = true\n",
    "master = false\n",
    "expect-workers = 0\n",
    "host = http://127.0.0.1:2288/invocations\n",
    "users = 10\n",
    "spawn-rate = 1\n",
    "run-time = 5m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca99e5b-feb2-49c8-b794-1bcb34ffe6a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Result - Distil Bert"
   ]
  },
  {
   "cell_type": "raw",
   "id": "740809e5-868c-42d5-bb0d-df07fe2a6124",
   "metadata": {},
   "source": [
    "Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
    "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
    "POST     /invocations                                                                     966     0(0.00%) |     89      55     221     92 |    3.23        0.00\n",
    "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
    "         Aggregated                                                                       966     0(0.00%) |     89      55     221     92 |    3.23        0.00\n",
    "\n",
    "Response time percentiles (approximated)\n",
    "Type     Name                                                                                  50%    66%    75%    80%    90%    95%    98%    99%  99.9% 99.99%   100% # reqs\n",
    "--------|--------------------------------------------------------------------------------|--------|------|------|------|------|------|------|------|------|------|------|------\n",
    "POST     /invocations                                                                           92     97    100    100    120    130    150    160    220    220    220    966\n",
    "--------|--------------------------------------------------------------------------------|--------|------|------|------|------|------|------|------|------|------|------|------\n",
    "         Aggregated                                                                             92     97    100    100    120    130    150    160    220    220    220    966"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdcb629-b20d-4502-bf78-7e93b6d14c3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Result - Bert"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0946561-ec0a-4030-b6b6-6ea795c36c78",
   "metadata": {},
   "source": [
    "Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
    "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
    "POST     /invocations                                                                     940     0(0.00%) |    171     105     444    150 |    3.14        0.00\n",
    "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
    "         Aggregated                                                                       940     0(0.00%) |    171     105     444    150 |    3.14        0.00\n",
    "\n",
    "Response time percentiles (approximated)\n",
    "Type     Name                                                                                  50%    66%    75%    80%    90%    95%    98%    99%  99.9% 99.99%   100% # reqs\n",
    "--------|--------------------------------------------------------------------------------|--------|------|------|------|------|------|------|------|------|------|------|------\n",
    "POST     /invocations                                                                          150    170    190    210    250    290    350    370    440    440    440    940\n",
    "--------|--------------------------------------------------------------------------------|--------|------|------|------|------|------|------|------|------|------|------|------\n",
    "         Aggregated                                                                            150    170    190    210    250    290    350    370    440    440    440    940"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eed63d5b-f958-4e0d-8502-b2f7b5563045",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "04ad9a18-5467-4a81-8af4-a23a5693e171",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4aa9f97-7203-42de-95ae-6ba4d2569fc0",
   "metadata": {},
   "source": [
    "### Config File"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4104fecd-4c5e-4bfe-a949-3c66b0295aaa",
   "metadata": {},
   "source": [
    "# master.conf in current directory\n",
    "locustfile = locust_asc.py\n",
    "headless = true\n",
    "master = false\n",
    "expect-workers = 0\n",
    "host = http://127.0.0.1:2288/invocations\n",
    "users = 20\n",
    "spawn-rate = 2\n",
    "run-time = 5m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46780fa-0357-4b24-950d-a55ab4dcd647",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Result - Distil Bert"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9664f26e-a1a4-4bc5-8f3a-8f8dd750e1a4",
   "metadata": {},
   "source": [
    "Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
    "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
    "POST     /invocations                                                                    1930     0(0.00%) |     95      54     285     88 |    6.44        0.00\n",
    "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
    "         Aggregated                                                                      1930     0(0.00%) |     95      54     285     88 |    6.44        0.00\n",
    "\n",
    "Response time percentiles (approximated)\n",
    "Type     Name                                                                                  50%    66%    75%    80%    90%    95%    98%    99%  99.9% 99.99%   100% # reqs\n",
    "--------|--------------------------------------------------------------------------------|--------|------|------|------|------|------|------|------|------|------|------|------\n",
    "POST     /invocations                                                                           88    100    110    120    150    170    200    220    280    290    290   1930\n",
    "--------|--------------------------------------------------------------------------------|--------|------|------|------|------|------|------|------|------|------|------|------\n",
    "         Aggregated                                                                             88    100    110    120    150    170    200    220    280    290    290   1930"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283d2c15-d307-4db4-84b5-39b5330f8575",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Result - Bert"
   ]
  },
  {
   "cell_type": "raw",
   "id": "226b2ec5-bcfa-49cb-9117-d9719300bf0f",
   "metadata": {},
   "source": [
    "Type     Name                                                                          # reqs      # fails |    Avg     Min     Max    Med |   req/s  failures/s\n",
    "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
    "POST     /invocations                                                                    1834     0(0.00%) |    233     105     914    200 |    6.12        0.00\n",
    "--------|----------------------------------------------------------------------------|-------|-------------|-------|-------|-------|-------|--------|-----------\n",
    "         Aggregated                                                                      1834     0(0.00%) |    233     105     914    200 |    6.12        0.00\n",
    "\n",
    "Response time percentiles (approximated)\n",
    "Type     Name                                                                                  50%    66%    75%    80%    90%    95%    98%    99%  99.9% 99.99%   100% # reqs\n",
    "--------|--------------------------------------------------------------------------------|--------|------|------|------|------|------|------|------|------|------|------|------\n",
    "POST     /invocations                                                                          200    250    290    320    400    490    590    660    900    910    910   1834\n",
    "--------|--------------------------------------------------------------------------------|--------|------|------|------|------|------|------|------|------|------|------|------\n",
    "         Aggregated                                                                            200    250    290    320    400    490    590    660    900    910    910   1834"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
